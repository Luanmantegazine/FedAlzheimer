{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Luanmantegazine/TF/blob/main/FED_ADNIData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "seGN9Ju7v7w-"
   },
   "source": [
    "import torch\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib as plt\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from imblearn.over_sampling import SMOTE\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlknLHbixiCq"
   },
   "source": [
    "#Definindo o número de clientes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HtpTQpfMxhka",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b9877add-b320-4f89-e65b-53f73d5f1e20"
   },
   "source": [
    "num_users = 3\n",
    "num_classes =  4\n",
    "epochs = 100\n",
    "frac = 1\n",
    "lr= 0.0001\n",
    "batch_size = 64\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIgHK_5Cx0Xo"
   },
   "source": [
    "#Importando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8TDdVPq2x4cf"
   },
   "source": [
    "data_path = '/Users/luanr/pycharm/TF/Alzheimers Dataset'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=data_path + '/train',\n",
    "    transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.RandomPerspective(),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=data_path + '/test',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def count_samples_per_class(dataset):\n",
    "    class_counts = {}\n",
    "    for class_idx, class_name in enumerate(dataset.classes):\n",
    "        class_counts[class_name] = len([label for _, label in dataset.samples if label == class_idx])\n",
    "    return class_counts\n",
    "\n",
    "print(\"Número de amostras por classe no treino:\")\n",
    "train_class_counts = count_samples_per_class(train_dataset)\n",
    "for class_name, count in train_class_counts.items():\n",
    "    print(f\"{class_name}: {count}\")\n",
    "\n",
    "print(\"Número de amostras por classe no teste:\")\n",
    "test_class_counts = count_samples_per_class(test_dataset)\n",
    "for class_name, count in test_class_counts.items():\n",
    "    print(f\"{class_name}: {count}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzTpOB0SyjLK"
   },
   "source": [
    "#Definindo o treinamento do cliente"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_Xcv6TMcynpJ"
   },
   "source": [
    "class ResNet50_client_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50_client_side, self).__init__()\n",
    "        self.layer1 = nn.Sequential (\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.layer2 = self._make_layer(64, 64, 3)\n",
    "\n",
    "    def _make_layer(self, in_planes, out_planes, blocks):\n",
    "        layers = []\n",
    "        layers.append(self._make_bottleneck(in_planes, out_planes, stride=1))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(self._make_bottleneck(out_planes * 4, out_planes, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_bottleneck(self, in_planes, planes, stride=1):\n",
    "        expansion = 4\n",
    "        out_planes = planes * expansion\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, planes, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(planes, out_planes, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual1 = self.layer1(x)\n",
    "        out1 = self.layer2(residual1)\n",
    "        out1 = F.relu(out1) if out1.size(1) == residual1.size(1) else nn.Conv2d(out1.size(1), residual1.size(1), kernel_size=1, bias=False).to(out1.device)(out1)\n",
    "        out1 += residual1\n",
    "        residual2 = F.relu(out1)\n",
    "        return residual2\n",
    "\n",
    "net_glob_client = ResNet50_client_side()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We use\", torch.cuda.device_count(), \"GPUs\")\n",
    "    net_glob_client = nn.DataParallel(net_glob_client)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPhaspYryuye"
   },
   "source": [
    "#Definindo o treinamento do servidor"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LapoJhY6yq28"
   },
   "source": [
    "class Baseblock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, input_planes, planes, stride=1, dim_change=None):\n",
    "        super(Baseblock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_planes, planes, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dim_change = dim_change\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = F.relu(self.bn2(self.conv2(output)))\n",
    "        output = self.bn3(self.conv3(output))\n",
    "\n",
    "        if self.dim_change is not None:\n",
    "            res = self.dim_change(res)\n",
    "\n",
    "        output += res\n",
    "        output = F.relu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResNet50_server_side(nn.Module):\n",
    "    def __init__(self, block, num_layers, num_classes=4):\n",
    "        super(ResNet50_server_side, self).__init__()\n",
    "        self.input_planes = 64\n",
    "        self.conv1 = nn.Conv2d(64, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        dim_change = None\n",
    "        if stride != 1 or self.input_planes != planes * block.expansion:\n",
    "            dim_change = nn.Sequential(\n",
    "                nn.Conv2d(self.input_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.input_planes, planes, stride, dim_change))\n",
    "        self.input_planes = planes * block.expansion\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.input_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net_glob_server = ResNet50_server_side(Baseblock, [3, 4, 6, 3], num_classes=4)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We use\", torch.cuda.device_count(), \"GPUs\")\n",
    "    net_glob_server = nn.DataParallel(net_glob_server)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jiPpKWagy5OH"
   },
   "source": [
    "batch_acc_train = []\n",
    "batch_loss_train = []\n",
    "batch_precision_train = []\n",
    "batch_recall_train = []\n",
    "acc_train_collect_user = []\n",
    "loss_train_collect_user = []\n",
    "loss_test_collect_user = []\n",
    "batch_auc_train = []\n",
    "batch_acc_test = []\n",
    "batch_loss_test = []\n",
    "batch_prec_test = []\n",
    "batch_recall_test = []\n",
    "loss_train_collect = [[] for _ in range(num_users)]\n",
    "acc_train_collect = [[] for _ in range(num_users)]\n",
    "loss_test_collect = [[] for _ in range(num_users)]\n",
    "acc_test_collect = [[] for _ in range(num_users)]\n",
    "auc_train_collect = [[] for _ in range(num_users)]\n",
    "auc_test_collect = [[] for _ in range(num_users)]\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r48wsNx9y7VM"
   },
   "source": [
    "def FedAvg(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
    "    return w_avg\n",
    "\n",
    "def calculate_metrics(fx, y, num_classes):\n",
    "    preds = fx.argmax(dim=1)\n",
    "    accuracy = torchmetrics.functional.accuracy(preds, y, task='multiclass', num_classes=num_classes)\n",
    "    precision = torchmetrics.functional.precision(preds, y, average='macro', task='multiclass', num_classes=num_classes)\n",
    "    recall = torchmetrics.functional.recall(preds, y, average='macro', task='multiclass', num_classes=num_classes)\n",
    "    f1 = torchmetrics.functional.f1_score(preds, y, average='macro', task='multiclass', num_classes=num_classes)\n",
    "    auc = torchmetrics.functional.auroc(F.softmax(fx, dim=1), y, task='multiclass', num_classes=num_classes)\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "w_glob_server = net_glob_server.state_dict()\n",
    "w_locals_server = []\n",
    "\n",
    "idx_collect = []\n",
    "l_epoch_check = False\n",
    "fed_check = False\n",
    "\n",
    "net_model_server = [ResNet50_server_side(Baseblock, [3, 4, 6, 3], num_classes) for i in range(num_users)]\n",
    "\n",
    "net_server = copy.deepcopy(net_model_server[0]).to(device)\n",
    "optimizer_server = torch.optim.Adam(net_server.parameters(), lr=lr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ybaVUIqZy-1j"
   },
   "source": [
    "def train_server(fx_client, y, l_epoch_count, l_epoch, idx, len_batch, num_classes=4):\n",
    "    global net_model_server, criterion, optimizer_server, device\n",
    "    global batch_acc_train, batch_loss_train, batch_precision_train, batch_recall_train, batch_auc_train\n",
    "    global l_epoch_check, fed_check, loss_train_collect, acc_train_collect, count1\n",
    "    global acc_avg_all_user_train, loss_avg_all_user_train, idx_collect, w_locals_server, w_glob_server, net_server\n",
    "    global loss_train_collect_user, acc_train_collect_user, lr\n",
    "\n",
    "    net_server = net_model_server[idx].to(device)\n",
    "    net_server.train()\n",
    "    optimizer_server = torch.optim.Adam(net_server.parameters(), lr=lr)\n",
    "\n",
    "    optimizer_server.zero_grad()\n",
    "    fx_client = fx_client.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    fx_server = net_server(fx_client)\n",
    "    loss = criterion(fx_server, y)\n",
    "    acc, precision, recall, f1, auc = calculate_metrics(fx_server, y, num_classes)\n",
    "\n",
    "    loss.backward()\n",
    "    dfx_client = fx_client.grad.clone().detach()\n",
    "    optimizer_server.step()\n",
    "\n",
    "    batch_loss_train.append(loss.item())\n",
    "    batch_acc_train.append(acc.item())\n",
    "    batch_precision_train.append(precision.item())\n",
    "    batch_recall_train.append(recall.item())\n",
    "    batch_auc_train.append(auc.item())\n",
    "\n",
    "    net_model_server[idx] = net_server\n",
    "\n",
    "    count1 += 1\n",
    "    if count1 == len_batch:\n",
    "        acc_avg_train = sum(batch_acc_train) / len(batch_acc_train)\n",
    "        loss_avg_train = sum(batch_loss_train) / len(batch_loss_train)\n",
    "        precision_avg_train = sum(batch_precision_train) / len(batch_precision_train)\n",
    "        recall_avg_train = sum(batch_recall_train) / len(batch_recall_train)\n",
    "        auc_avg_train = sum(batch_auc_train) / len(batch_auc_train)\n",
    "\n",
    "        batch_acc_train.clear()\n",
    "        batch_loss_train.clear()\n",
    "        batch_precision_train.clear()\n",
    "        batch_recall_train.clear()\n",
    "        batch_auc_train.clear()\n",
    "        count1 = 0\n",
    "\n",
    "        print(f'Client{idx} Train => Local Epoch: {l_epoch_count} \\tAcc: {acc_avg_train:.3f} \\tLoss: {loss_avg_train:.4f} \\tPrecision: {precision_avg_train:.3f} \\tRecall: {recall_avg_train:.3f} \\tAUC: {auc_avg_train:.3f}')\n",
    "\n",
    "        w_server = net_server.state_dict()\n",
    "\n",
    "        if l_epoch_count == l_epoch - 1:\n",
    "            l_epoch_check = True\n",
    "            w_locals_server.append(copy.deepcopy(w_server))\n",
    "\n",
    "            acc_avg_train_all = acc_avg_train\n",
    "            loss_avg_train_all = loss_avg_train\n",
    "\n",
    "            loss_train_collect_user.append(loss_avg_train_all)\n",
    "            acc_train_collect_user.append(acc_avg_train_all)\n",
    "\n",
    "            if idx not in idx_collect:\n",
    "                idx_collect.append(idx)\n",
    "\n",
    "        if len(idx_collect) == num_users:\n",
    "            fed_check = True\n",
    "            w_glob_server = FedAvg(w_locals_server)\n",
    "            net_glob_server.load_state_dict(w_glob_server)\n",
    "            net_model_server = [copy.deepcopy(net_glob_server) for _ in range(num_users)]\n",
    "            w_locals_server.clear()\n",
    "            idx_collect.clear()\n",
    "\n",
    "            acc_avg_all_user_train = sum(acc_train_collect_user) / len(acc_train_collect_user)\n",
    "            loss_avg_all_user_train = sum(loss_train_collect_user) / len(loss_train_collect_user)\n",
    "\n",
    "            loss_train_collect.append(loss_avg_all_user_train)\n",
    "            acc_train_collect.append(acc_avg_all_user_train)\n",
    "\n",
    "            acc_train_collect_user.clear()\n",
    "            loss_train_collect_user.clear()\n",
    "\n",
    "    return dfx_client"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-azC4sjfzBte",
    "ExecuteTime": {
     "end_time": "2024-08-10T17:37:47.172597Z",
     "start_time": "2024-08-10T17:37:47.166045Z"
    }
   },
   "source": [
    "def evaluate_server(fx_client, y, idx, len_batch, ell, num_classes=4):\n",
    "    global net_model_server, criterion, batch_acc_test, batch_loss_test, check_fed, net_server, net_glob_server\n",
    "    global loss_test_collect, acc_test_collect, count2, num_users, acc_avg_train_all, loss_avg_train_all, w_glob_server, l_epoch_check, fed_check\n",
    "    global loss_test_collect_user, acc_test_collect_user, acc_avg_all_user_train, loss_avg_all_user_train\n",
    "\n",
    "    batch_precision_test = []\n",
    "    acc_test_collect_user = []\n",
    "    batch_recall_test = []\n",
    "    batch_auc_test = []\n",
    "    batch_f1_test = []\n",
    "\n",
    "    net = copy.deepcopy(net_model_server[idx]).to(device)\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fx_client = fx_client.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        fx_server = net(fx_client)\n",
    "        loss = criterion(fx_server, y)\n",
    "\n",
    "        acc, precision, recall, f1, auc = calculate_metrics(fx_server, y, num_classes)\n",
    "\n",
    "        batch_loss_test.append(loss.item())\n",
    "        batch_acc_test.append(acc.item())\n",
    "        batch_precision_test.append(precision.item())\n",
    "        batch_recall_test.append(recall.item())\n",
    "        batch_auc_test.append(auc.item())\n",
    "        batch_f1_test.append(f1.item())\n",
    "\n",
    "        count2 += 1\n",
    "        if count2 == len_batch:\n",
    "            acc_avg_test = sum(batch_acc_test) / len(batch_acc_test)\n",
    "            loss_avg_test = sum(batch_loss_test) / len(batch_loss_test)\n",
    "            precision_avg_test = sum(batch_precision_test) / len(batch_precision_test)\n",
    "            recall_avg_test = sum(batch_recall_test) / len(batch_recall_test)\n",
    "            auc_avg_test = sum(batch_auc_test) / len(batch_auc_test)\n",
    "            f1_avg_test = sum(batch_f1_test) / len(batch_f1_test)\n",
    "\n",
    "            batch_acc_test = []\n",
    "            batch_loss_test = []\n",
    "            batch_precision_test = []\n",
    "            batch_recall_test = []\n",
    "            batch_auc_test = []\n",
    "            batch_f1_test = []\n",
    "            count2 = 0\n",
    "\n",
    "            print('Client{} Test =>                   \\tAcc: {:.3f} \\tLoss: {:.4f} \\tPrecision: {:.3f} \\tRecall: {:.3f} \\tAUC: {:.3f} \\tF1-Score: {:.3f}'.format(\n",
    "                idx, acc_avg_test, loss_avg_test, precision_avg_test, recall_avg_test, auc_avg_test, f1_avg_test))\n",
    "\n",
    "            if l_epoch_check:\n",
    "                l_epoch_check = False\n",
    "\n",
    "                acc_avg_test_all = acc_avg_test\n",
    "                loss_avg_test_all = loss_avg_test\n",
    "\n",
    "                loss_test_collect_user.append(loss_avg_test_all)\n",
    "                acc_test_collect_user.append(acc_avg_test_all)\n",
    "\n",
    "            if fed_check:\n",
    "                fed_check = False\n",
    "                print(\"------------------------------------------------\")\n",
    "                print(\"------ Federation process at Server-Side ------- \")\n",
    "                print(\"------------------------------------------------\")\n",
    "\n",
    "                acc_avg_all_user = sum(acc_test_collect_user) / len(acc_test_collect_user)\n",
    "                loss_avg_all_user = sum(loss_test_collect_user) / len(loss_test_collect_user)\n",
    "\n",
    "                loss_test_collect.append(loss_avg_all_user)\n",
    "                acc_test_collect.append(acc_avg_all_user)\n",
    "                acc_test_collect_user = []\n",
    "                loss_test_collect_user = []\n",
    "\n",
    "                print(\"====================== SERVER V1==========================\")\n",
    "                print(' Train: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user_train, loss_avg_all_user_train))\n",
    "                print(' Test: Round {:3d}, Avg Accuracy {:.3f} | Avg Loss {:.3f}'.format(ell, acc_avg_all_user, loss_avg_all_user))\n",
    "                print(\"==========================================================\")\n",
    "    return"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "82R9R7c6zK3x"
   },
   "source": [
    "def evaluate_accuracy(net, loader, device, return_conf_matrix=False, num_classes=4):\n",
    "    net.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    \n",
    "    all_preds = torch.tensor(all_preds)\n",
    "    all_labels = torch.tensor(all_labels)\n",
    "\n",
    "    \n",
    "    accuracy = torchmetrics.functional.accuracy(all_preds, all_labels, task='multiclass', num_classes=num_classes).item()\n",
    "    precision = torchmetrics.functional.precision(all_preds, all_labels, average='macro', task='multiclass', num_classes=num_classes).item()\n",
    "    recall = torchmetrics.functional.recall(all_preds, all_labels, average='macro', task='multiclass', num_classes=num_classes).item()\n",
    "    f1 = torchmetrics.functional.f1_score(all_preds, all_labels, average='macro', task='multiclass', num_classes=num_classes).item()\n",
    "    auc = torchmetrics.functional.auroc(F.softmax(outputs, dim=1), all_labels, task='multiclass', num_classes=num_classes).item()\n",
    "\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels.cpu().numpy(), all_preds.cpu().numpy())\n",
    "\n",
    "    if return_conf_matrix:\n",
    "        return accuracy, precision, recall, f1, auc, conf_matrix\n",
    "    else:\n",
    "        return accuracy, precision, recall, f1, auc\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, label\n",
    "\n",
    "class Client(object):\n",
    "    def __init__(self, net_client_model, idx, lr, device, train_loader=None, test_loader=None, idxs=None, idxs_test=None):\n",
    "        self.idx = idx\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.local_ep = 1\n",
    "        self.ldr_train = train_loader\n",
    "        self.ldr_test = test_loader\n",
    "\n",
    "    def train(self, net):\n",
    "        net.train()\n",
    "        optimizer_client = torch.optim.Adam(net.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer_client, step_size=30, gamma=0.1)\n",
    "\n",
    "        for iter in range(self.local_ep):\n",
    "            len_batch = len(self.ldr_train)\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                optimizer_client.zero_grad()\n",
    "                fx = net(images)\n",
    "                client_fx = fx.clone().detach().requires_grad_(True)\n",
    "                dfx = train_server(client_fx, labels, iter, self.local_ep, self.idx, len_batch)\n",
    "                fx.backward(dfx)\n",
    "                optimizer_client.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        return net.state_dict()\n",
    "\n",
    "    def evaluate(self, net, ell, num_classes=4):\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            len_batch = len(self.ldr_test)\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_test):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                fx = net(images)\n",
    "                evaluate_server(fx, labels, self.idx, len_batch, ell, num_classes)\n",
    "\n",
    "        return\n",
    "\n",
    "def dataset_iid(dataset, num_users):\n",
    "    num_items = int(len(dataset) / num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Treino!\n"
   ],
   "metadata": {
    "id": "I50k6NHR8V9x"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epoch_times = []\n",
    "\n",
    "dict_users = dataset_iid(train_dataset, num_users)\n",
    "dict_users_test = dataset_iid(test_dataset, num_users)\n",
    "net_glob_client.train()\n",
    "\n",
    "w_glob_client = net_glob_client.state_dict()\n",
    "\n",
    "\n",
    "for iter in range(epochs):\n",
    "    start_time = time.time()\n",
    "    m = max(int(frac * num_users), 1)\n",
    "    idxs_users = np.random.choice(range(num_users), m, replace=False)\n",
    "    w_locals_client = []\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        local = Client(\n",
    "            net_glob_client,\n",
    "            idx,\n",
    "            lr,\n",
    "            device,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            idxs=dict_users[idx],\n",
    "            idxs_test=dict_users_test[idx]\n",
    "        )\n",
    "\n",
    "        w_client = local.train(net=copy.deepcopy(net_glob_client).to(device))\n",
    "        w_locals_client.append(copy.deepcopy(w_client))\n",
    "\n",
    "        local.evaluate(net=copy.deepcopy(net_glob_client).to(device), ell=iter)\n",
    "\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"------ FedServer: Federation process at Client-Side ------- \")\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "\n",
    "    w_glob_client = FedAvg(w_locals_client)\n",
    "    net_glob_client.load_state_dict(w_glob_client)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    print(f\"Epoch {iter + 1}/{epochs} - Time taken: {epoch_time:.2f} seconds\")\n",
    "\n",
    "print(\"Training and Evaluation completed!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "mount_file_id": "1Cjudpa5t47y1peQIuUWnPJ4F1B-C57Z5",
   "authorship_tag": "ABX9TyNfp+CtEj1OSf2X18Wu68ru",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
