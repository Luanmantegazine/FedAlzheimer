{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-28T01:03:02.661146Z",
     "start_time": "2025-07-28T01:02:27.670747Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics.functional as F_tm\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_path = '/Users/luanr/pycharm/FedAlzheimer/FedAlzheimer/data/ADNI 4'"
   ],
   "id": "916ca4012a743d1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RandomNoise(object):\n",
    "    def __init__(self, p=0.5, mean=0., std=0.1): self.p,self.mean,self.std=p,mean,std\n",
    "    def __call__(self, tensor):\n",
    "        if torch.rand(1).item() < self.p: return tensor + torch.randn(tensor.size(), device=tensor.device) * self.std + self.mean\n",
    "        return tensor\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}(p={self.p}, mean={self.mean}, std={self.std})\"\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None): self.subset, self.transform = subset, transform\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.subset[index]\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return image, label\n",
    "    def __len__(self): return len(self.subset)\n",
    "\n",
    "class ResNet50_client_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if name.startswith((\"conv1\", \"bn1\")): param.requires_grad = False\n",
    "        self.features = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1, resnet.layer2)\n",
    "    def forward(self, x): return self.features(x)\n",
    "\n",
    "class ResNet50_server_side(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(resnet.layer3, resnet.layer4)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        num_ftrs = resnet.fc.in_features\n",
    "        self.classifier = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(num_ftrs, num_classes))\n",
    "    def forward(self, x): x=self.features(x); x=self.pool(x); x=torch.flatten(x,1); return self.classifier(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, client_model, server_model): super().__init__();self.client_model=client_model;self.server_model=server_model\n",
    "    def forward(self, x): return self.server_model(self.client_model(x))\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self,patience=10,verbose=True,delta=0,save_path='chkp.pt'): self.patience,self.verbose,self.delta,self.save_path=patience,verbose,delta,save_path; self.counter,self.best_score,self.early_stop,self.val_loss_min=0,None,False,np.Inf\n",
    "    def __call__(self,val_loss,models):\n",
    "        score=-val_loss; net_client, net_server = models\n",
    "        if self.best_score is None: self.best_score=score; self.save_checkpoint(val_loss, net_client, net_server)\n",
    "        elif score<self.best_score+self.delta:\n",
    "            self.counter+=1\n",
    "            if self.verbose:print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter>=self.patience: self.early_stop=True\n",
    "        else: self.best_score=score; self.save_checkpoint(val_loss, net_client, net_server); self.counter=0\n",
    "    def save_checkpoint(self,val_loss,net_client,net_server):\n",
    "        if self.verbose:print(f'Validation loss decreased ({self.val_loss_min:.6f}-->{val_loss:.6f}). Saving model to {self.save_path}')\n",
    "        torch.save({'net_glob_client_state_dict':net_client.state_dict(),'net_glob_server_state_dict':net_server.state_dict()}, self.save_path); self.val_loss_min=val_loss\n",
    "\n",
    "def FedAvg(w_locals):\n",
    "    if not w_locals:\n",
    "        return {}\n",
    "\n",
    "    w_avg = copy.deepcopy(w_locals[0])\n",
    "\n",
    "    for k in w_avg.keys():\n",
    "        if not w_avg[k].dtype.is_floating_point:\n",
    "            continue\n",
    "\n",
    "        stacked_tensors = torch.stack([w[k].float() for w in w_locals], dim=0)\n",
    "        w_avg[k] = stacked_tensors.mean(dim=0)\n",
    "\n",
    "    return w_avg\n",
    "\n",
    "def calculate_metrics(fx, y, num_classes):\n",
    "    preds=fx.argmax(dim=1)\n",
    "    acc=F_tm.accuracy(preds,y,task='multiclass',num_classes=num_classes)\n",
    "    prec=F_tm.precision(preds,y,average='macro',task='multiclass',num_classes=num_classes)\n",
    "    rec=F_tm.recall(preds,y,average='macro',task='multiclass',num_classes=num_classes)\n",
    "    f1=F_tm.f1_score(preds,y,average='macro',task='multiclass',num_classes=num_classes)\n",
    "    try: auc=F_tm.auroc(F.softmax(fx,dim=1),y,task='multiclass',num_classes=num_classes)\n",
    "    except ValueError: auc=torch.tensor(0.0, device=fx.device)\n",
    "    return acc,prec,rec,f1,auc\n",
    "\n",
    "def train_server(fx_client, y, net_server, optimizer_server, criterion, num_classes):\n",
    "    net_server.train();optimizer_server.zero_grad()\n",
    "    fx_server=net_server(fx_client);loss=criterion(fx_server,y)\n",
    "    metrics=calculate_metrics(fx_server,y,num_classes)\n",
    "    loss.backward();dfx_client=fx_client.grad.clone().detach();optimizer_server.step()\n",
    "    return dfx_client,loss.item(),metrics\n",
    "\n",
    "def evaluate_server(fx_client, y, net_server, criterion, num_classes):\n",
    "    net_server.eval()\n",
    "    with torch.no_grad():\n",
    "        fx_server=net_server(fx_client);loss=criterion(fx_server,y)\n",
    "        metrics=calculate_metrics(fx_server,y,num_classes)\n",
    "    return loss.item(),metrics\n",
    "\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, idx, lr, device, idxs, idxs_test, net_server_initial_weights, weight_decay):\n",
    "        self.idx,self.device,self.lr,self.local_ep=idx,device,lr,1\n",
    "        self.train_dataset=CustomDataset(Subset(full_dataset,list(idxs)),transform=train_transform)\n",
    "        self.test_dataset=CustomDataset(Subset(full_dataset,list(idxs_test)),transform=test_transform)\n",
    "        self.ldr_train=DataLoader(self.train_dataset,batch_size=batch_size,shuffle=True,num_workers=2,pin_memory=True)\n",
    "        self.ldr_test=DataLoader(self.test_dataset,batch_size=batch_size,shuffle=False,num_workers=2,pin_memory=True)\n",
    "        self.net_server_local_copy=ResNet50_server_side(num_classes=num_classes).to(self.device)\n",
    "        self.net_server_local_copy.load_state_dict(net_server_initial_weights)\n",
    "        self.optimizer_server_local=torch.optim.Adam(self.net_server_local_copy.parameters(),lr=self.lr,weight_decay=weight_decay)\n",
    "        self.mu = mu\n",
    "\n",
    "    def train(self, net_client):\n",
    "        net_client.train()\n",
    "        net_client.to(self.device)\n",
    "\n",
    "        global_weights = copy.deepcopy(net_client.state_dict())\n",
    "\n",
    "        optimizer_client = torch.optim.Adam(net_client.parameters(), lr=self.lr)\n",
    "\n",
    "        batch_losses, batch_metrics = [], []\n",
    "        for images, labels in self.ldr_train:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            optimizer_client.zero_grad()\n",
    "\n",
    "            fx = net_client(images)\n",
    "            client_fx = fx.clone().detach().requires_grad_(True)\n",
    "\n",
    "            dfx, loss, metrics = train_server(client_fx, labels, self.net_server_local_copy, self.optimizer_server_local, criterion, num_classes)\n",
    "\n",
    "            fx.backward(dfx)\n",
    "\n",
    "            proximal_term = 0.0\n",
    "            for name, param in net_client.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    proximal_term += torch.sum(torch.pow(param - global_weights[name].to(self.device), 2))\n",
    "\n",
    "            prox_loss = (self.mu / 2) * proximal_term\n",
    "            prox_loss.backward()\n",
    "            # -------------------------\n",
    "\n",
    "            optimizer_client.step()\n",
    "\n",
    "            batch_losses.append(loss)\n",
    "            batch_metrics.append([m.item() for m in metrics])\n",
    "\n",
    "        avg_loss = sum(batch_losses) / len(batch_losses) if batch_losses else 0\n",
    "        avg_metrics = [sum(col) / len(col) for col in zip(*batch_metrics)] if batch_metrics else [0]*5\n",
    "\n",
    "        return net_client.state_dict(), self.net_server_local_copy.state_dict(), avg_loss, avg_metrics\n",
    "\n",
    "    def evaluate(self, net_client, net_server):\n",
    "        net_client.eval();net_server.eval();net_client.to(self.device);net_server.to(self.device)\n",
    "        batch_losses,batch_metrics=[],[]\n",
    "        with torch.no_grad():\n",
    "            for images,labels in self.ldr_test:\n",
    "                images,labels=images.to(self.device),labels.to(self.device)\n",
    "                fx=net_client(images)\n",
    "                loss,metrics=evaluate_server(fx,labels,net_server,criterion,num_classes)\n",
    "                batch_losses.append(loss);batch_metrics.append([m.item() for m in metrics])\n",
    "        avg_loss=sum(batch_losses)/len(batch_losses) if batch_losses else 0\n",
    "        avg_metrics=[sum(col)/len(col) for col in zip(*batch_metrics)] if batch_metrics else [0]*5\n",
    "        return avg_loss,avg_metrics\n",
    "\n",
    "def evaluate_accuracy(net,loader,device,return_conf_matrix=False,num_classes=3):\n",
    "    net.eval();all_preds,all_labels,all_outputs=[],[],[]\n",
    "    with torch.no_grad():\n",
    "        for images,labels in loader:\n",
    "            images,labels=images.to(device),labels.to(device);outputs=net(images);_,predicted=torch.max(outputs,1);all_outputs.append(outputs.cpu());all_preds.extend(predicted.cpu().numpy());all_labels.extend(labels.cpu().numpy())\n",
    "    all_outputs=torch.cat(all_outputs,dim=0);all_preds=torch.tensor(all_preds);all_labels=torch.tensor(all_labels);\n",
    "    accuracy=F_tm.accuracy(all_preds,all_labels,task='multiclass',num_classes=num_classes).item();precision=F_tm.precision(all_preds,all_labels,average='macro',task='multiclass',num_classes=num_classes).item();recall=F_tm.recall(all_preds,all_labels,average='macro',task='multiclass',num_classes=num_classes).item();f1=F_tm.f1_score(all_preds,all_labels,average='macro',task='multiclass',num_classes=num_classes).item();auc=F_tm.auroc(F.softmax(all_outputs,dim=1),all_labels,task=\"multiclass\",num_classes=num_classes).item();\n",
    "    conf_matrix=confusion_matrix(all_labels.cpu().numpy(),all_preds.cpu().numpy())\n",
    "    if return_conf_matrix:return accuracy,precision,recall,f1,auc,conf_matrix\n",
    "    return accuracy,precision,recall,f1,auc\n",
    "\n",
    "def dataset_iid(indices, num_users):\n",
    "    indices = np.random.permutation(indices); splits = np.array_split(indices, num_users); return {i: set(splits[i]) for i in range(num_users)}\n"
   ],
   "id": "4d0c4e9b1ac29638"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.85,1.15)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    RandomNoise(p=0.2, mean=0., std=0.08),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "test_transform=transforms.Compose([transforms.Resize((224,224)),transforms.Grayscale(num_output_channels=3),transforms.ToTensor(),transforms.Normalize([0.5]*3,[0.5]*3)])\n",
    "full_dataset=datasets.ImageFolder(root=data_path)\n",
    "labels_all=[lbl for _,lbl in full_dataset.samples]\n",
    "train_idx,test_idx=train_test_split(np.arange(len(full_dataset)),test_size=0.2,stratify=labels_all,random_state=SEED)\n",
    "main_train_dataset=CustomDataset(Subset(full_dataset,train_idx),transform=train_transform)\n",
    "main_test_dataset = CustomDataset(Subset(full_dataset, test_idx), transform=test_transform)"
   ],
   "id": "fd246702a4f904e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "client_counts = [3, 5, 7, 9]\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "\n",
    "for num_users in client_counts:\n",
    "    print(f\"\\n{'='*80}\\nINICIANDO TREINAMENTO PARA {num_users} CLIENTES\\n{'='*80}\\n\")\n",
    "\n",
    "    epochs,frac,lr,weight_decay=80,1.0,1e-4,1e-4\n",
    "    mu = 0.01\n",
    "    net_glob_client=ResNet50_client_side().to(device);net_glob_server=ResNet50_server_side(num_classes=num_classes).to(device)\n",
    "    w_glob_client=net_glob_client.state_dict();w_glob_server=net_glob_server.state_dict()\n",
    "\n",
    "    # Reinicializa listas de histórico de métricas\n",
    "    history={'train_loss':[],'train_acc':[],'test_loss':[],'test_acc':[],'train_recall':[],'test_recall':[],'train_auc':[],'test_auc':[]}\n",
    "\n",
    "    dict_users=dataset_iid(train_idx,num_users);dict_users_test=dataset_iid(test_idx,num_users)\n",
    "    labels = [label for _, label in main_train_dataset]\n",
    "    class_weights=compute_class_weight('balanced',classes=np.unique(labels),y=labels)\n",
    "    criterion=nn.CrossEntropyLoss(weight=torch.tensor(class_weights,dtype=torch.float).to(device))\n",
    "\n",
    "    checkpoint_path=f'best_model_{num_users}_clients.pt'\n",
    "    early_stopping=EarlyStopping(patience=10,verbose=True,save_path=checkpoint_path)\n",
    "\n",
    "    # --- LOOP DE TREINAMENTO (ESTRUTURA REATORADA) ---\n",
    "    for iter_epoch in range(epochs):\n",
    "        start_time=time.time()\n",
    "        idxs_users=np.random.choice(range(num_users),max(int(frac*num_users),1),replace=False)\n",
    "\n",
    "        w_locals_client,w_locals_server,round_train_loss,round_train_metrics=[],[],[],[]\n",
    "\n",
    "        # ETAPA 1: TREINAMENTO LOCAL DOS CLIENTES\n",
    "        for idx in idxs_users:\n",
    "            local=Client(idx,lr,device,dict_users[idx],dict_users_test[idx],w_glob_server,weight_decay)\n",
    "            w_c,w_s,t_loss,t_metrics=local.train(net_client=copy.deepcopy(net_glob_client))\n",
    "            w_locals_client.append(w_c);w_locals_server.append(w_s);round_train_loss.append(t_loss);round_train_metrics.append(t_metrics)\n",
    "\n",
    "        # ETAPA 2: AGREGAÇÃO NO SERVIDOR (FedAvg)\n",
    "        w_glob_client = FedAvg(w_locals_client)\n",
    "        w_glob_server = FedAvg(w_locals_server)\n",
    "        net_glob_client.load_state_dict(w_glob_client)\n",
    "        net_glob_server.load_state_dict(w_glob_server)\n",
    "\n",
    "        # ETAPA 3: AVALIAÇÃO GLOBAL E COLETA DE MÉTRICAS\n",
    "        round_test_loss,round_test_metrics=[],[]\n",
    "        for idx in idxs_users:\n",
    "            local=Client(idx,lr,device,dict_users[idx],dict_users_test[idx],w_glob_server,weight_decay)\n",
    "            test_loss,test_metrics=local.evaluate(net_glob_client,net_glob_server)\n",
    "            round_test_loss.append(test_loss);round_test_metrics.append(test_metrics)\n",
    "\n",
    "        # Calcula médias da rodada e salva no histórico\n",
    "        history['train_loss'].append(sum(round_train_loss)/len(round_train_loss))\n",
    "        avg_train_metrics=[sum(col)/len(col) for col in zip(*round_train_metrics)]\n",
    "        history['train_acc'].append(avg_train_metrics[0]);history['train_recall'].append(avg_train_metrics[2]);history['train_auc'].append(avg_train_metrics[4])\n",
    "\n",
    "        history['test_loss'].append(sum(round_test_loss)/len(round_test_loss))\n",
    "        avg_test_metrics=[sum(col)/len(col) for col in zip(*round_test_metrics)]\n",
    "        history['test_acc'].append(avg_test_metrics[0]);history['test_recall'].append(avg_test_metrics[2]);history['test_auc'].append(avg_test_metrics[4])\n",
    "\n",
    "        print(f\"Epoch {iter_epoch+1}/{epochs} | Train Loss:{history['train_loss'][-1]:.4f} Acc:{history['train_acc'][-1]:.4f} | Test Loss:{history['test_loss'][-1]:.4f} Acc:{history['test_acc'][-1]:.4f} | Time:{time.time()-start_time:.2f}s\")\n",
    "\n",
    "        early_stopping(history['test_loss'][-1],(net_glob_client,net_glob_server))\n",
    "        if early_stopping.early_stop:print(\"Early stopping triggered.\");break\n",
    "\n",
    "    print(f\"\\nTreinamento para {num_users} clientes concluído!\")\n",
    "\n",
    "    # --- AVALIAÇÃO E PLOTAGEM FINAL ---\n",
    "    print(\"\\nGerando gráficos e avaliação final...\")\n",
    "    if history['train_acc']:\n",
    "        epochs_range=range(1,len(history['train_acc'])+1)\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,7));fig.suptitle(f'Acurácia e Perda ({num_users} Clientes)',fontsize=20)\n",
    "        ax1.plot(epochs_range,history['train_acc'],'o-',label='Acurácia de Treino');ax1.plot(epochs_range,history['test_acc'],'s-',label='Acurácia de Validação');ax1.set_title('Acurácia vs. Época');ax1.set_xlabel('Época');ax1.set_ylabel('Acurácia');ax1.legend()\n",
    "        ax2.plot(epochs_range,history['train_loss'],'o-',label='Perda de Treino');ax2.plot(epochs_range,history['test_loss'],'s-',label='Perda de Validação');ax2.set_title('Perda vs. Época');ax2.set_xlabel('Época');ax2.set_ylabel('Perda');ax2.legend()\n",
    "        plt.show()\n",
    "        fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,7));fig.suptitle(f'Recall e AUC ({num_users} Clientes)',fontsize=20)\n",
    "        ax1.plot(epochs_range,history['train_recall'],'o-',label='Recall de Treino');ax1.plot(epochs_range,history['test_recall'],'s-',label='Recall de Validação');ax1.set_title('Recall vs. Época');ax1.set_xlabel('Época');ax1.set_ylabel('Recall');ax1.legend()\n",
    "        ax2.plot(epochs_range,history['train_auc'],'o-',label='AUC de Treino');ax2.plot(epochs_range,history['test_auc'],'s-',label='AUC de Validação');ax2.set_title('AUC vs. Época');ax2.set_xlabel('Época');ax2.set_ylabel('AUC');ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "    try:\n",
    "        checkpoint=torch.load(checkpoint_path);net_glob_client.load_state_dict(checkpoint['net_glob_client_state_dict']);net_glob_server.load_state_dict(checkpoint['net_glob_server_state_dict'])\n",
    "        final_model=CombinedModel(net_glob_client,net_glob_server).to(device)\n",
    "        accuracy,precision,recall,f1,auc,conf_matrix=evaluate_accuracy(final_model,DataLoader(main_test_dataset,batch_size=batch_size,shuffle=False),device,return_conf_matrix=True,num_classes=num_classes)\n",
    "        print(f\"\\n--- Métricas Finais ({num_users} Clientes) ---\");print(f\"  Acurácia:{accuracy:.4f}, Precisão:{precision:.4f}, Recall:{recall:.4f}, F1-Score:{f1:.4f}, AUC:{auc:.4f}\")\n",
    "        plt.figure(figsize=(10,8));sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Blues',xticklabels=range(num_classes),yticklabels=range(num_classes));plt.title(f'Matriz de Confusão ({num_users} Clientes)',fontsize=18);plt.xlabel('Rótulo Previsto');plt.ylabel('Rótulo Verdadeiro');plt.show()\n",
    "    except FileNotFoundError:print(f\"Arquivo de checkpoint '{checkpoint_path}' não encontrado.\")\n",
    "    except Exception as e:print(f\"Ocorreu um erro na avaliação final: {e}\")"
   ],
   "id": "8ce669fb0d4dd8be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
