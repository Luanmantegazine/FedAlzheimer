{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-28T01:03:02.661146Z",
     "start_time": "2025-07-28T01:02:27.670747Z"
    }
   },
   "source": [
    "import os, copy, random, math, time, glob\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchmetrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_users = 3\n",
    "num_classes =  3\n",
    "epochs = 80\n",
    "frac = 1\n",
    "lr= 1e-4\n",
    "local_epochs   = 1\n",
    "batch_size = 32\n",
    "weight_decay =  5e-4\n",
    "mu = 0.1\n",
    "clip_grad      = 1.0   \n",
    "server_lr       = 0.7\n",
    "server_momentum = 0.8        \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "916ca4012a743d1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_path='/content/drive/MyDrive/ADNI'\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=data_path)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(full_dataset)),\n",
    "    test_size=0.20,\n",
    "    random_state=SEED,\n",
    "    stratify=full_dataset.targets\n",
    ")\n",
    "train_subset = Subset(full_dataset, train_idx)\n",
    "test_subset  = Subset(full_dataset, test_idx)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset, self.transform = subset, transform\n",
    "    def __getitem__(self, i):\n",
    "        img, y = self.subset[i]\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, y\n",
    "    def __len__(self): return len(self.subset)\n",
    "\n",
    "train_loader = DataLoader(CustomDataset(train_subset, train_tf),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(CustomDataset(test_subset,  test_tf ),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "labels_train = [full_dataset.targets[i] for i in train_idx]\n",
    "cls_weights  = compute_class_weight('balanced',\n",
    "                                    classes=np.unique(labels_train),\n",
    "                                    y=labels_train)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(cls_weights,\n",
    "                                                    dtype=torch.float,\n",
    "                                                    device=device))"
   ],
   "id": "4d0c4e9b1ac29638"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def FedAvg(w):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(1, len(w)): w_avg[k] += w[i][k]\n",
    "        w_avg[k] = torch.div(w_avg[k], len(w))\n",
    "    return w_avg"
   ],
   "id": "fd246702a4f904e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def FedAvgM(w_locals, sizes, w_glob, v_prev, beta=0.9, server_lr=1.0):\n",
    "    total = sum(sizes)\n",
    "    w_avg = {k: torch.zeros_like(p) for k,p in w_locals[0].items()\n",
    "             if torch.is_floating_point(p)}\n",
    "\n",
    "    for w,s in zip(w_locals, sizes):\n",
    "        for k in w_avg:\n",
    "            w_avg[k] += w[k] * (s / total)\n",
    "\n",
    "    new_state = {}\n",
    "    for k, p_glob in w_glob.items():\n",
    "        if not torch.is_floating_point(p_glob):\n",
    "            new_state[k] = p_glob           \n",
    "            continue\n",
    "        delta = w_avg[k] - p_glob\n",
    "        v_prev[k] = beta * v_prev.get(k, torch.zeros_like(delta)) + delta\n",
    "        new_state[k] = p_glob + server_lr * v_prev[k]\n",
    "\n",
    "    return new_state, v_prev"
   ],
   "id": "8ce669fb0d4dd8be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def reset_bn_stats(model, loader):\n",
    "    model.train()\n",
    "    for x,_ in loader: model(x.to(device))\n",
    "    model.eval()\n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluate_global(model, loader):\n",
    "    model.eval()\n",
    "    loss, logits_all, y_all, pred_all = 0.0, [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss += criterion(out, y).item() * x.size(0)\n",
    "        logits_all.append(F.softmax(out, 1).cpu())\n",
    "        y_all.extend(y.cpu().tolist())\n",
    "        pred_all.extend(out.argmax(1).cpu().tolist())\n",
    "\n",
    "    y_t   = torch.tensor(y_all)\n",
    "    pred_t= torch.tensor(pred_all)\n",
    "    logit_t = torch.cat(logits_all)\n",
    "    kwargs = {'task':'multiclass', 'num_classes':num_classes}\n",
    "\n",
    "    acc = torchmetrics.functional.accuracy(pred_t, y_t, **kwargs).item()\n",
    "    f1  = torchmetrics.functional.f1_score(pred_t, y_t, average='macro', **kwargs).item()\n",
    "    auc = torchmetrics.functional.auroc(logit_t, y_t, **kwargs).item()\n",
    "    return loss/len(loader.dataset), acc, f1, auc"
   ],
   "id": "bcad806e807fa465"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ResNet50_client_side(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class ResNet50_server_side(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.layer3,\n",
    "            resnet.layer4,\n",
    "            resnet.avgpool\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(resnet.fc.in_features, num_classes) \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, client, server):\n",
    "        super().__init__(); self.client, self.server = client, server\n",
    "    def forward(self, x): return self.server(self.client(x))\n",
    "    \n",
    "class Client:\n",
    "    def __init__(self, idx, train_ids, test_ids, net_c_glob, net_s_glob):\n",
    "        self.device = device\n",
    "        tr_sub = Subset(train_subset.dataset,\n",
    "                        [train_subset.indices[i] for i in train_ids])\n",
    "        te_sub = Subset(test_subset.dataset,\n",
    "                        [test_subset.indices[i] for i in test_ids])\n",
    "        self.tr_loader = DataLoader(CustomDataset(tr_sub, train_tf),\n",
    "                                    batch_size=batch_size, shuffle=True)\n",
    "        self.te_loader = DataLoader(CustomDataset(te_sub, test_tf),\n",
    "                                    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.net_c = copy.deepcopy(net_c_glob).to(self.device)\n",
    "        self.net_s = copy.deepcopy(net_s_glob).to(self.device)\n",
    "\n",
    "        self.opt_c = torch.optim.Adam(self.net_c.parameters(),\n",
    "                                      lr=lr, weight_decay=weight_decay)\n",
    "        self.opt_s = torch.optim.Adam(self.net_s.parameters(),\n",
    "                                      lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    def train(self):\n",
    "        self.net_c.train(); self.net_s.train()\n",
    "        loss_sum, correct, total = 0.0, 0, 0\n",
    "        for _ in range(local_epochs):\n",
    "            for x, y in self.tr_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                self.opt_c.zero_grad(); self.opt_s.zero_grad()\n",
    "                out = self.net_s(self.net_c(x))\n",
    "                loss = criterion(out, y)\n",
    "                loss.backward()\n",
    "\n",
    "                if mu>0:\n",
    "                    with torch.no_grad():\n",
    "                        for pg, pl in zip(net_glob_c.parameters(),\n",
    "                                          self.net_c.parameters()):\n",
    "                            pl.grad.add_(mu * (pl - pg.to(self.device)))\n",
    "\n",
    "                if clip_grad:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.net_c.parameters(),\n",
    "                                                   clip_grad)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.net_s.parameters(),\n",
    "                                                   clip_grad)\n",
    "\n",
    "                self.opt_c.step(); self.opt_s.step()\n",
    "\n",
    "                loss_sum += loss.item() * x.size(0)\n",
    "                correct  += (out.argmax(1)==y).sum().item()\n",
    "                total    += y.size(0)\n",
    "\n",
    "        return (self.net_c.state_dict(), self.net_s.state_dict(),\n",
    "                loss_sum/total, correct/total)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, net_c_glob, net_s_glob):\n",
    "        model = CombinedModel(net_c_glob, net_s_glob).to(self.device)\n",
    "        return evaluate_global(model, self.te_loader)[:2]\n",
    "\n",
    "def dataset_noniid(idx_array, num_users, alpha=0.5):\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    labels = np.array(full_dataset.targets)[idx_array]\n",
    "    idx = np.arange(len(idx_array))\n",
    "    dict_users = {i: [] for i in range(num_users)}\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        idx_cls = idx[labels == c]\n",
    "        rng.shuffle(idx_cls)\n",
    "        proportions = rng.dirichlet(np.repeat(alpha, num_users))\n",
    "        split_points = (np.cumsum(proportions) * len(idx_cls)).astype(int)[:-1]\n",
    "        splits = np.split(idx_cls, split_points)\n",
    "        for i, part in enumerate(splits):\n",
    "            dict_users[i].extend(part.tolist())\n",
    "\n",
    "    for i in dict_users: rng.shuffle(dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "dict_users_train = dataset_noniid(train_idx, num_users)\n",
    "dict_users_test  = dataset_noniid(test_idx,  num_users)\n",
    "\n"
   ],
   "id": "e95260264cadfbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "net_glob_c = ResNet50_client_side().to(device)\n",
    "net_glob_s = ResNet50_server_side(num_classes).to(device)\n",
    "\n",
    "v_c = {k: torch.zeros_like(p) for k,p in net_glob_c.state_dict().items()}\n",
    "v_s = {k: torch.zeros_like(p) for k,p in net_glob_s.state_dict().items()}\n",
    "\n",
    "w_c_locals, w_s_locals, sizes_local = [], [], []\n",
    "\n",
    "client_metrics = {m:{i:[] for i in range(num_users)}\n",
    "                  for m in ('train_loss','train_acc','test_loss','test_acc')}\n",
    "server_metrics = {m:[] for m in ('train_loss','train_acc','test_loss','test_acc')}\n",
    "\n",
    "print(\"\\n===== Iniciando treinamento (FedAvgM) =====\")\n",
    "for rnd in range(1, epochs+1):\n",
    "    print(f\"\\n--- Rodada {rnd}/{epochs} ---\")\n",
    "    selected = np.random.choice(range(num_users),\n",
    "                                max(int(frac*num_users),1), replace=False)\n",
    "    w_c_locals, w_s_locals = [], []\n",
    "\n",
    "    for idx in selected:\n",
    "        user = Client(idx,\n",
    "                      dict_users_train[idx], dict_users_test[idx],\n",
    "                      net_glob_c, net_glob_s)\n",
    "        w_c, w_s, l, a = user.train()\n",
    "        w_c_locals.append(w_c); w_s_locals.append(w_s)\n",
    "        sizes_local.append(len(dict_users_train[idx]))\n",
    "        client_metrics['train_loss'][idx].append(l)\n",
    "        client_metrics['train_acc' ][idx].append(a)\n",
    "        print(f\"Cliente {idx}: loss={l:.4f} acc={a:.4f}\")\n",
    "\n",
    "    # --- Agregação FedAvgM ---------------------------------------------------\n",
    "    new_c, v_c = FedAvgM(w_c_locals, sizes_local,\n",
    "                     net_glob_c.state_dict(), v_c,\n",
    "                     beta=server_momentum, server_lr=server_lr)\n",
    "    new_s, v_s = FedAvgM(w_s_locals, sizes_local,\n",
    "                     net_glob_s.state_dict(), v_s,\n",
    "                     beta=server_momentum, server_lr=server_lr)\n",
    "    net_glob_c.load_state_dict(new_c)\n",
    "    net_glob_s.load_state_dict(new_s)\n",
    "    reset_bn_stats(CombinedModel(net_glob_c, net_glob_s), train_loader)\n",
    "    print(\"Agregação (FedAvgM) concluída.\")\n",
    "\n",
    "\n",
    "    g_model = CombinedModel(net_glob_c.eval(), net_glob_s.eval())\n",
    "    tr_l, tr_a, _, _ = evaluate_global(g_model, train_loader)\n",
    "    te_l, te_a, _, _ = evaluate_global(g_model, test_loader)\n",
    "    server_metrics['train_loss'].append(tr_l)\n",
    "    server_metrics['train_acc' ].append(tr_a)\n",
    "    server_metrics['test_loss' ].append(te_l)\n",
    "    server_metrics['test_acc' ].append(te_a)\n",
    "    print(f\"[Servidor] TrainAcc:{tr_a:.4f}  TestAcc:{te_a:.4f}\")\n",
    "\n",
    "    # --- Avaliação local pós‑agregação ---------------------------------------\n",
    "    for idx in range(num_users):\n",
    "        if idx not in selected:\n",
    "            client_metrics['train_loss'][idx].append(None)\n",
    "            client_metrics['train_acc' ][idx].append(None)\n",
    "        c_l, c_a = Client(idx,\n",
    "                          dict_users_train[idx], dict_users_test[idx],\n",
    "                          net_glob_c, net_glob_s).evaluate(net_glob_c, net_glob_s)\n",
    "        client_metrics['test_loss'][idx].append(c_l)\n",
    "        client_metrics['test_acc' ][idx].append(c_a)\n",
    "\n",
    "print(\"\\n===== Treinamento concluído =====\")"
   ],
   "id": "66db204e68572239"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
