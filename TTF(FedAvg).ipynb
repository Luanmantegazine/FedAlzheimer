{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luanmantegazine/FedAlzheimer/blob/main/TTF(FedAvg).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "d3Exonuq6qst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359b50ee-fddd-4260-acb7-e5ff0738e18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=2.12 in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: tensorflow-federated>=0.71.0 in /usr/local/lib/python3.11/dist-packages (0.84.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (22.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (1.14.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.12) (2.14.0)\n",
            "Requirement already satisfied: attrs~=23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (23.1.0)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (5.5.2)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (0.1.8)\n",
            "Requirement already satisfied: dp-accounting==0.4.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (0.4.3)\n",
            "Requirement already satisfied: google-vizier==0.1.11 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (0.1.11)\n",
            "Requirement already satisfied: jaxlib==0.4.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (0.4.14)\n",
            "Requirement already satisfied: jax==0.4.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (0.4.14)\n",
            "Requirement already satisfied: portpicker~=1.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (1.6.0)\n",
            "Requirement already satisfied: scipy~=1.9.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (1.9.3)\n",
            "Requirement already satisfied: tensorflow-model-optimization==0.7.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (0.7.5)\n",
            "Requirement already satisfied: tensorflow-privacy==0.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (0.9.0)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (4.67.1)\n",
            "Requirement already satisfied: googleapis-common-protos==1.61.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated>=0.71.0) (1.61.0)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.11/dist-packages (from dp-accounting==0.4.3->tensorflow-federated>=0.71.0) (1.3.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.35.0 in /usr/local/lib/python3.11/dist-packages (from google-vizier==0.1.11->tensorflow-federated>=0.71.0) (1.62.3)\n",
            "Requirement already satisfied: sqlalchemy<=1.4.20,>=1.4 in /usr/local/lib/python3.11/dist-packages (from google-vizier==0.1.11->tensorflow-federated>=0.71.0) (1.4.20)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated>=0.71.0) (1.6.1)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated>=0.71.0) (0.22.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated>=0.71.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated>=0.71.0) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.12) (0.45.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from portpicker~=1.6->tensorflow-federated>=0.71.0) (5.9.5)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.12) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.12) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.12) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.12) (3.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.12) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.12) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.12) (2025.8.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated>=0.71.0) (3.2.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated>=0.71.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated>=0.71.0) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow>=2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.12) (3.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow>=2.12\" \"tensorflow-federated>=0.71.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i6IDwuebw1B",
        "outputId": "9f4fab5a-2c4e-43d8-a36e-444dac8b26d1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.11/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (22.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "rKZf3N4n4rjm"
      },
      "outputs": [],
      "source": [
        "import os, math, random, json, itertools\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict, Counter\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "try:\n",
        "    import nest_asyncio, asyncio\n",
        "    nest_asyncio.apply()\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPeZNGj_7hcm"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "cQI1XkY-7l0Q"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "\n",
        "  data_path: str = \"/content/drive/MyDrive/TCC - Grupo SLD/Projeto 2/ADNI\"\n",
        "\n",
        "  img_size: int = 224\n",
        "  grayscale_to_3: bool = True\n",
        "  per_image_standardization: bool = True\n",
        "  num_clients: int = 5\n",
        "  alpha_dirichlet: float = 0.5\n",
        "  test_size: float = 0.2\n",
        "  seed: int = 42\n",
        "\n",
        "  batch_size: int = 16\n",
        "  local_epochs: int = 1\n",
        "  rounds: int = 15\n",
        "  clients_per_round: int = 0  # 0 => usa todos\n",
        "\n",
        "  # Otimizadores (TFF)\n",
        "  client_lr: float = 1e-3\n",
        "  client_momentum: float = 0.9\n",
        "  server_lr: float = 1.0\n",
        "\n",
        "  # Perda\n",
        "  label_smoothing: float = 0.05\n",
        "  use_focal_loss: bool = False\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_all(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)\n",
        "seed_all(cfg.seed)\n"
      ],
      "metadata": {
        "id": "r5dxGvu5Z-rs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_EXTS = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\")\n",
        "\n",
        "def discover_imagefolder(root: str):\n",
        "    if not os.path.isdir(root):\n",
        "        raise FileNotFoundError(f\"data_path inexistente: {root}\")\n",
        "    classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
        "    if not classes:\n",
        "        raise ValueError(f\"Nenhuma subpasta de classe encontrada em {root}\")\n",
        "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "    idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
        "    filepaths, labels = [], []\n",
        "    for c in classes:\n",
        "        cdir = os.path.join(root, c)\n",
        "        for dp, _, fns in os.walk(cdir):\n",
        "            for f in fns:\n",
        "                if f.lower().endswith(IMG_EXTS):\n",
        "                    filepaths.append(os.path.join(dp, f))\n",
        "                    labels.append(class_to_idx[c])\n",
        "    if not filepaths:\n",
        "        raise ValueError(\"Nenhuma imagem encontrada.\")\n",
        "    return filepaths, labels, idx_to_class, class_to_idx\n",
        "\n",
        "def extract_subject_id(path_str: str) -> str:\n",
        "    base = os.path.basename(path_str)\n",
        "    stem = os.path.splitext(base)[0]\n",
        "    return stem.split(\"_\")[0] if \"_\" in stem else stem"
      ],
      "metadata": {
        "id": "ILppn7CsaEUt"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_subject_split(filepaths, labels, test_size: float, seed: int):\n",
        "    subject_to_indices = defaultdict(list)\n",
        "    for idx, fp in enumerate(filepaths):\n",
        "        sid = extract_subject_id(fp)\n",
        "        subject_to_indices[sid].append(idx)\n",
        "\n",
        "    subject_label = {}\n",
        "    for sid, idcs in subject_to_indices.items():\n",
        "        maj = Counter([labels[i] for i in idcs]).most_common(1)[0][0]\n",
        "        subject_label[sid] = maj\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    subjects_by_class = defaultdict(list)\n",
        "    for sid, c in subject_label.items():\n",
        "        subjects_by_class[c].append(sid)\n",
        "\n",
        "    train_subjects, test_subjects = [], []\n",
        "    for c, sids in subjects_by_class.items():\n",
        "        s = sids[:]; rng.shuffle(s)\n",
        "        n = len(s); n_test = max(1, int(round(test_size * n))) if n > 1 else 1\n",
        "        if n - n_test == 0 and n > 1: n_test -= 1\n",
        "        test_subjects.extend(s[:n_test]); train_subjects.extend(s[n_test:])\n",
        "\n",
        "    train_idx, test_idx = [], []\n",
        "    for sid in train_subjects:\n",
        "        train_idx.extend(subject_to_indices[sid])\n",
        "    for sid in test_subjects:\n",
        "        test_idx.extend(subject_to_indices[sid])\n",
        "\n",
        "    return train_idx, test_idx, subject_label, subject_to_indices\n",
        "\n",
        "def dirichlet_partition_by_subjects(train_subjects_by_class, alpha: float, num_clients: int, seed: int):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    client_to_subjects = {i: [] for i in range(num_clients)}\n",
        "    for c, subj_list in train_subjects_by_class.items():\n",
        "        subj_list = subj_list[:]; rng.shuffle(subj_list); n = len(subj_list)\n",
        "        if n == 0: continue\n",
        "        prop = rng.dirichlet(alpha=[alpha]*num_clients)\n",
        "        counts = np.floor(prop * n).astype(int)\n",
        "        diff = n - counts.sum()\n",
        "        if diff > 0:\n",
        "            rem = prop * n - counts\n",
        "            for i in np.argsort(rem)[-diff:]: counts[i] += 1\n",
        "        elif diff < 0:\n",
        "            for i in np.argsort(prop)[:abs(diff)]:\n",
        "                if counts[i] > 0: counts[i] -= 1\n",
        "        s = 0\n",
        "        for cid, k in enumerate(counts):\n",
        "            if k <= 0: continue\n",
        "            client_to_subjects[cid].extend(subj_list[s:s+k]); s += k\n",
        "\n",
        "    empties = [cid for cid, lst in client_to_subjects.items() if not lst]\n",
        "    avail = [cid for cid, lst in client_to_subjects.items() if len(lst) > 1]\n",
        "    for cid in empties:\n",
        "        if not avail: break\n",
        "        donor = avail.pop()\n",
        "        moved = client_to_subjects[donor].pop()\n",
        "        client_to_subjects[cid].append(moved)\n",
        "        if len(client_to_subjects[donor]) > 1: avail.append(donor)\n",
        "    return client_to_subjects"
      ],
      "metadata": {
        "id": "tGjyS1tKaF5Z"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths, labels, idx_to_class, class_to_idx = discover_imagefolder(cfg.data_path)\n",
        "num_classes = len(idx_to_class)\n",
        "print(f\"[INFO] {len(filepaths)} imagens | classes: {idx_to_class}\")\n",
        "\n",
        "train_idx, test_idx, subject_label, subject_to_indices = stratified_subject_split(\n",
        "    filepaths, labels, cfg.test_size, cfg.seed\n",
        ")\n",
        "\n",
        "train_subjects_by_class = defaultdict(list)\n",
        "for sid, c in subject_label.items():\n",
        "    if subject_to_indices[sid][0] in train_idx:\n",
        "        train_subjects_by_class[c].append(sid)\n",
        "\n",
        "client_to_subjects = dirichlet_partition_by_subjects(\n",
        "    train_subjects_by_class, cfg.alpha_dirichlet, cfg.num_clients, cfg.seed\n",
        ")\n",
        "\n",
        "client_to_files = {i: [] for i in range(cfg.num_clients)}\n",
        "for cid, sids in client_to_subjects.items():\n",
        "    for sid in sids:\n",
        "        for i in subject_to_indices[sid]:\n",
        "            if i in train_idx:\n",
        "                client_to_files[cid].append((filepaths[i], labels[i]))\n",
        "\n",
        "test_files = [(filepaths[i], labels[i]) for i in test_idx]\n",
        "print(f\"[INFO] Treino total: {sum(len(v) for v in client_to_files.values())} | Teste: {len(test_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBtsnEftaKIb",
        "outputId": "5fa972d4-58b9-4e90-890d-a7bd09513776"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 545 imagens | classes: {0: 'AD', 1: 'CN', 2: 'MCI'}\n",
            "[INFO] Treino total: 157 | Teste: 388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def _decode(fp, training: bool):\n",
        "    img_bytes = tf.io.read_file(fp)\n",
        "    img = tf.io.decode_image(img_bytes, channels=1 if cfg.grayscale_to_3 else 3, expand_animations=False)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, [cfg.img_size, cfg.img_size])\n",
        "    if cfg.grayscale_to_3:\n",
        "        img = tf.image.grayscale_to_rgb(img)\n",
        "    if cfg.per_image_standardization:\n",
        "        img = tf.image.per_image_standardization(img)\n",
        "    return img\n",
        "\n",
        "def build_tfds(files_and_labels: List[Tuple[str, int]], training: bool) -> tf.data.Dataset:\n",
        "    if not files_and_labels:\n",
        "        ds = tf.data.Dataset.from_tensors((tf.zeros([cfg.img_size, cfg.img_size, 3], tf.float32), 0))\n",
        "        return ds.batch(cfg.batch_size).map(lambda x,y: {'x': x, 'y': tf.cast(y, tf.int32)})\n",
        "    fpaths = tf.constant([fp for fp,_ in files_and_labels])\n",
        "    labs   = tf.constant([y  for _,y in files_and_labels], dtype=tf.int32)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((fpaths, labs))\n",
        "    if training:\n",
        "        ds = ds.shuffle(min(len(files_and_labels), 1000), seed=cfg.seed, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(lambda fp, y: (_decode(fp, training), y), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(cfg.batch_size, drop_remainder=False).prefetch(AUTOTUNE)\n",
        "    ds = ds.map(lambda x, y: {'x': x, 'y': y})\n",
        "    if training and cfg.local_epochs > 1:\n",
        "        ds = ds.repeat(cfg.local_epochs)\n",
        "    return ds\n",
        "\n",
        "test_ds = build_tfds(test_files, training=False)"
      ],
      "metadata": {
        "id": "Vlxm0-OvaOW0"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_keras_model(num_classes: int) -> tf.keras.Model:\n",
        "    inp = tf.keras.Input(shape=(cfg.img_size, cfg.img_size, 3))\n",
        "    x = tf.keras.layers.RandomRotation(0.05)(inp)\n",
        "    x = tf.keras.layers.RandomTranslation(0.02, 0.02)(x)\n",
        "    x = tf.keras.layers.RandomZoom(0.02, 0.02)(x)\n",
        "    base = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(base.output)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    out = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return tf.keras.Model(inputs=inp, outputs=out)\n",
        "\n",
        "class SparseCategoricalFocalLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma=2.0, alpha=None, label_smoothing=0.0, name=\"sparse_categorical_focal_loss\"):\n",
        "        super().__init__(name=name)\n",
        "        self.gamma, self.alpha, self.label_smoothing = gamma, alpha, label_smoothing\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_true_oh = tf.one_hot(y_true, depth=num_classes, dtype=y_pred.dtype)\n",
        "        if self.label_smoothing > 0.0:\n",
        "            y_true_oh = y_true_oh * (1.0 - self.label_smoothing) + self.label_smoothing / num_classes\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-8, 1.0)  # modelo já sai com softmax\n",
        "        pt = tf.reduce_sum(y_true_oh * y_pred, axis=-1)\n",
        "        alpha_factor = 1.0\n",
        "        if self.alpha is not None:\n",
        "            alpha_vec = tf.constant(self.alpha, dtype=y_pred.dtype)\n",
        "            alpha_factor = tf.reduce_sum(y_true_oh * alpha_vec, axis=-1)\n",
        "        loss = -alpha_factor * tf.pow(1.0 - pt, self.gamma) * tf.math.log(pt)\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "class SparseCCEWithLabelSmoothing(tf.keras.losses.Loss):\n",
        "    def __init__(self, label_smoothing=0.1, name=\"sparse_cce_with_ls\"):\n",
        "        super().__init__(name=name)\n",
        "        self.label_smoothing = float(label_smoothing)\n",
        "        self._cce = tf.keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=False, label_smoothing=label_smoothing\n",
        "        )\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_true_oh = tf.one_hot(y_true, depth=num_classes, dtype=y_pred.dtype)\n",
        "        return self._cce(y_true_oh, y_pred)\n",
        "\n",
        "def make_loss():\n",
        "    if cfg.use_focal_loss:\n",
        "        return SparseCategoricalFocalLoss(label_smoothing=cfg.label_smoothing)\n",
        "    else:\n",
        "        if cfg.label_smoothing > 0.0:\n",
        "            return SparseCCEWithLabelSmoothing(label_smoothing=cfg.label_smoothing)\n",
        "        else:\n",
        "            return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n"
      ],
      "metadata": {
        "id": "NkT6IatoaeEp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_sample_client = 0\n",
        "sample_ds = build_tfds(client_to_files[_sample_client], training=True)\n",
        "input_spec = sample_ds.element_spec\n",
        "\n",
        "def model_fn_tff():\n",
        "    keras_model = build_keras_model(num_classes)\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model=keras_model,\n",
        "        loss=make_loss(),\n",
        "        input_spec=input_spec,\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "WhrNF2ZyebjT"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_serializable(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: to_serializable(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return [to_serializable(v) for v in obj]\n",
        "    if isinstance(obj, tf.Tensor):\n",
        "        v = obj.numpy()\n",
        "        return v.item() if np.ndim(v) == 0 else v.tolist()\n",
        "    if isinstance(obj, (np.floating, np.integer)):\n",
        "        return obj.item()\n",
        "    return obj"
      ],
      "metadata": {
        "id": "5wryVTQAhjwF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn=model_fn_tff,\n",
        "    client_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=cfg.client_lr, momentum=cfg.client_momentum),\n",
        "    server_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=cfg.server_lr),\n",
        ")\n",
        "state = learning_process.initialize()\n",
        "\n",
        "client_ids = list(client_to_files.keys())\n",
        "clients_per_round = (min(cfg.clients_per_round, len(client_ids))\n",
        "                     if cfg.clients_per_round and cfg.clients_per_round > 0\n",
        "                     else len(client_ids))\n",
        "print(f\"[INFO] Rounds: {cfg.rounds} | clientes/rodada: {clients_per_round}/{len(client_ids)}\")\n",
        "\n",
        "for rnd in range(1, cfg.rounds + 1):\n",
        "    if clients_per_round < len(client_ids):\n",
        "        selected = random.sample(client_ids, k=clients_per_round)\n",
        "    else:\n",
        "        selected = client_ids\n",
        "    federated_train_data = [build_tfds(client_to_files[cid], training=True) for cid in selected]\n",
        "\n",
        "    state, metrics = learning_process.next(state, federated_train_data)\n",
        "    try:\n",
        "        metrics_dict = {k: float(v) if hasattr(v, \"numpy\") else v for k, v in metrics.items()}\n",
        "    except Exception:\n",
        "        metrics_dict = metrics\n",
        "    print(f\"[Round {rnd:02d}] train=\", json.dumps(to_serializable(metrics), indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et9JQoESaspv",
        "outputId": "2e2e657e-9789-4075-fa0a-3f06f7bfd795"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Rounds: 15 | clientes/rodada: 5/5\n",
            "[Round 01] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.23566879332065582,\n",
            "      \"loss\": 1.2202403545379639,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 02] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.35668790340423584,\n",
            "      \"loss\": 1.1421669721603394,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 03] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.5095541477203369,\n",
            "      \"loss\": 1.0746499300003052,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 04] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.5095541477203369,\n",
            "      \"loss\": 1.0688458681106567,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 05] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6114649772644043,\n",
            "      \"loss\": 1.0506439208984375,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 06] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6242038011550903,\n",
            "      \"loss\": 1.0059205293655396,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 07] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6242038011550903,\n",
            "      \"loss\": 1.0454148054122925,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 08] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6178343892097473,\n",
            "      \"loss\": 0.9965974688529968,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 09] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6050955653190613,\n",
            "      \"loss\": 1.014185905456543,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 10] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6369426846504211,\n",
            "      \"loss\": 0.956692636013031,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 11] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6242038011550903,\n",
            "      \"loss\": 0.9812992215156555,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 12] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6178343892097473,\n",
            "      \"loss\": 0.934552013874054,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 13] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6242038011550903,\n",
            "      \"loss\": 0.9407835602760315,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 14] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6050955653190613,\n",
            "      \"loss\": 1.0042755603790283,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n",
            "[Round 15] train= {\n",
            "  \"distributor\": [],\n",
            "  \"client_work\": {\n",
            "    \"train\": {\n",
            "      \"accuracy\": 0.6242038011550903,\n",
            "      \"loss\": 0.9570563435554504,\n",
            "      \"num_examples\": 157,\n",
            "      \"num_batches\": 12\n",
            "    }\n",
            "  },\n",
            "  \"aggregator\": {\n",
            "    \"mean_value\": [],\n",
            "    \"mean_weight\": []\n",
            "  },\n",
            "  \"finalizer\": {\n",
            "    \"update_non_finite\": 0\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = build_keras_model(num_classes)\n",
        "weights = learning_process.get_model_weights(state)\n",
        "try:\n",
        "    weights.assign_weights_to(final_model)\n",
        "except Exception:\n",
        "    # fallback (se necessário)\n",
        "    try:\n",
        "        final_model.set_weights(list(weights.trainable) + list(weights.non_trainable))\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Falha ao transferir pesos:\", e)\n",
        "\n",
        "final_model.save(\"tff_alzheimer_model.h5\")\n",
        "print(\"[INFO] Modelo salvo: tff_alzheimer_model.h5\")\n",
        "\n",
        "# Avaliação central detalhada (accuracy, macro-P/R/F1, CM)\n",
        "def evaluate_central(model: tf.keras.Model, files_and_labels: List[Tuple[str,int]]):\n",
        "    ds = build_tfds(files_and_labels, training=False)\n",
        "    y_true_all, y_pred_all = [], []\n",
        "    for batch in ds:\n",
        "        x, y = batch['x'], batch['y']\n",
        "        probs = model(x, training=False).numpy()\n",
        "        y_pred = probs.argmax(axis=-1)\n",
        "        y_true_all.append(y.numpy()); y_pred_all.append(y_pred)\n",
        "    y_true = np.concatenate(y_true_all, axis=0); y_pred = np.concatenate(y_pred_all, axis=0)\n",
        "    acc = float((y_true == y_pred).mean())\n",
        "    cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=num_classes).numpy()\n",
        "    prec_c, rec_c, f1_c = [], [], []\n",
        "    for c in range(num_classes):\n",
        "        tp = cm[c,c]; fp = cm[:,c].sum() - tp; fn = cm[c,:].sum() - tp\n",
        "        prec = tp / (tp + fp + 1e-12); rec = tp / (tp + fn + 1e-12)\n",
        "        f1 = 2*prec*rec / (prec + rec + 1e-12)\n",
        "        prec_c.append(float(prec)); rec_c.append(float(rec)); f1_c.append(float(f1))\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_precision\": float(np.mean(prec_c)),\n",
        "        \"macro_recall\": float(np.mean(rec_c)),\n",
        "        \"macro_f1\": float(np.mean(f1_c)),\n",
        "        \"per_class\": {\n",
        "            \"precision\": prec_c, \"recall\": rec_c, \"f1\": f1_c,\n",
        "            \"class_names\": [idx_to_class[i] for i in range(num_classes)]\n",
        "        },\n",
        "        \"confusion_matrix\": cm.tolist()\n",
        "    }\n",
        "\n",
        "central_eval = evaluate_central(final_model, test_files)\n",
        "print(\"[CENTRAL TEST] \", json.dumps({k:v for k,v in central_eval.items() if k!='confusion_matrix'}, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmBRBDDjk_TO",
        "outputId": "b4d3f3fe-978c-43d1-b041-74ee599d22bd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Modelo salvo: tff_alzheimer_model.h5\n",
            "[CENTRAL TEST]  {\n",
            "  \"accuracy\": 0.4329896907216495,\n",
            "  \"macro_precision\": 0.14432989690721612,\n",
            "  \"macro_recall\": 0.33333333333333137,\n",
            "  \"macro_f1\": 0.20143884892072197,\n",
            "  \"per_class\": {\n",
            "    \"precision\": [\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.43298969072164833\n",
            "    ],\n",
            "    \"recall\": [\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.9999999999999941\n",
            "    ],\n",
            "    \"f1\": [\n",
            "      0.0,\n",
            "      0.0,\n",
            "      0.604316546762166\n",
            "    ],\n",
            "    \"class_names\": [\n",
            "      \"AD\",\n",
            "      \"CN\",\n",
            "      \"MCI\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1J7AJXs6hHtRlw8dn0yfVJiR7f7WEMJUt",
      "authorship_tag": "ABX9TyNYuWpxPDta0oBbB8Gh74Ql",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}