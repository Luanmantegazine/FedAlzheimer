{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41TyiT50lGBk",
        "outputId": "68db4362-0666-48d4-95e6-f5591d9feaba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA1aCwgklCmx"
      },
      "outputs": [],
      "source": [
        "import os, json, copy, time, random\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics.functional as F_tm\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset, Subset, Sampler\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewSQcGz8vyZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b374b54b-ef0e-429c-e5eb-5844230798dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "SEED = 12\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ajuste para seu caminho\n",
        "data_path = '/content/drive/MyDrive/ADNI4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNpxS8FTv13Q"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "experiment_config = {\n",
        "    \"experiment_name\": \"SlitFedAlzheimer_FedAvg\",\n",
        "    \"threat_model\": {\n",
        "        \"adversary_type\": \"passive\",\n",
        "        \"knowledge\": \"white-box\",\n",
        "        \"compromised_clients\": 0.0,\n",
        "        \"goal\": \"inference\"\n",
        "    },\n",
        "    \"privacy\": {\n",
        "        \"use_dp\": False,\n",
        "        \"dp_epsilon\": None,\n",
        "        \"dp_delta\": None,\n",
        "        \"gradient_clipping\": None,\n",
        "        \"secure_aggregation\": False\n",
        "    },\n",
        "    \"reproducibility\": {\"seed\": SEED}\n",
        "}\n",
        "with open(\"artifacts/experiment_config.json\",\"w\") as f:\n",
        "    json.dump(experiment_config, f, indent=2)\n",
        "\n",
        "CM_EVERY = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8zSbkFWv2m8"
      },
      "outputs": [],
      "source": [
        "class RandomNoise(object):\n",
        "    def __init__(self, p=0.5, mean=0., std=0.1):\n",
        "        self.p,self.mean,self.std=p,mean,std\n",
        "    def __call__(self, tensor):\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return tensor + torch.randn(tensor.size(), device=tensor.device) * self.std + self.mean\n",
        "        return tensor\n",
        "    def __repr__(self): return f\"{self.__class__.__name__}(p={self.p}, mean={self.mean}, std={self.std})\"\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset, self.transform = subset, transform\n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.subset[index]\n",
        "        if self.transform: image = self.transform(image)\n",
        "        return image, label\n",
        "    def __len__(self): return len(self.subset)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.85,1.15)),\n",
        "    transforms.ToTensor(),\n",
        "    RandomNoise(p=0.2, mean=0., std=0.08),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3,[0.5]*3)\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=data_path)\n",
        "\n",
        "# Paths & labels\n",
        "all_samples = full_dataset.samples  # list of (path, class_idx)\n",
        "all_paths   = [p for p, _ in all_samples]\n",
        "labels_all  = np.array([lbl for _, lbl in all_samples], dtype=int)\n",
        "classes = full_dataset.classes\n",
        "num_classes = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbbfoIY8CDdP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "root = Path(data_path)\n",
        "_PATTERNS = [\n",
        "    re.compile(r'(\\d{3}_S_\\d{4})'),               # 002_S_0295\n",
        "    re.compile(r'ADNI[_-](\\d{3}_S_\\d{4})', re.I), # ADNI_002_S_0295\n",
        "    re.compile(r'PTID[_-]?(\\d{3}_S_\\d{4})', re.I)\n",
        "]\n",
        "\n",
        "def extract_patient_id_from_filename(path_str: str) -> str:\n",
        "    fname = Path(path_str).name\n",
        "    for pat in _PATTERNS:\n",
        "        m = pat.search(fname)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    # Fallback: if nothing matches, use the stem => one scan == one \"patient\"\n",
        "    return Path(fname).stem\n",
        "\n",
        "patient_ids = np.array([extract_patient_id_from_filename(p) for p in all_paths])\n",
        "IDX_TO_PATIENT = {i: patient_ids[i] for i in range(len(all_samples))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rfmtCnrv6Jh"
      },
      "outputs": [],
      "source": [
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "all_idx = np.arange(len(full_dataset))\n",
        "train_idx, test_idx = next(sgkf.split(all_idx, y=labels_all, groups=patient_ids))\n",
        "\n",
        "# Sanity: patient disjoint\n",
        "train_pats = set(patient_ids[train_idx]); test_pats = set(patient_ids[test_idx])\n",
        "assert train_pats.isdisjoint(test_pats), \"Leakage: patient appears in both train and test!\"\n",
        "\n",
        "# Datasets\n",
        "main_train_dataset = CustomDataset(Subset(full_dataset, train_idx), transform=train_transform)\n",
        "main_test_dataset  = CustomDataset(Subset(full_dataset, test_idx),  transform=test_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWDQz5nzv8zz"
      },
      "outputs": [],
      "source": [
        "def partition_by_patient_balanced_no_empty(idx_array, idx_to_patient, labels_all, desired_users, seed=1234):\n",
        "    \"\"\"\n",
        "    Patient-exclusive + class-balanced + no empty clients.\n",
        "    If too few patients, reduces the effective number of users automatically.\n",
        "    Returns (dict_users, eff_users).\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    pats = sorted({idx_to_patient[i] for i in idx_array})\n",
        "    if not pats:\n",
        "        return {c: set() for c in range(desired_users)}, 0\n",
        "\n",
        "    pat_to_idxs = defaultdict(list)\n",
        "    for i in idx_array:\n",
        "        pat_to_idxs[idx_to_patient[i]].append(i)\n",
        "\n",
        "    def maj_label(p):\n",
        "        cnt = Counter(labels_all[j] for j in pat_to_idxs[p])\n",
        "        return int(cnt.most_common(1)[0][0])\n",
        "\n",
        "    pats_by_class = defaultdict(list)\n",
        "    for p in pats:\n",
        "        pats_by_class[maj_label(p)].append(p)\n",
        "    for c in list(pats_by_class.keys()):\n",
        "        rng.shuffle(pats_by_class[c])\n",
        "\n",
        "    eff_users = min(desired_users, len(pats))\n",
        "    buckets = [set() for _ in range(eff_users)]\n",
        "\n",
        "    # round-robin by class for diversity\n",
        "    while any(pats_by_class.values()):\n",
        "        for c in list(pats_by_class.keys()):\n",
        "            if pats_by_class[c]:\n",
        "                for b in range(eff_users):\n",
        "                    if pats_by_class[c]:\n",
        "                        buckets[b].add(pats_by_class[c].pop())\n",
        "\n",
        "    # fix any empties by redistributing\n",
        "    all_pats = [p for b in buckets for p in b]\n",
        "    non_empty = [b for b in buckets if b]\n",
        "    if len(non_empty) < eff_users:\n",
        "        buckets = [set() for _ in range(len(non_empty) or 1)]\n",
        "        for k, p in enumerate(all_pats):\n",
        "            buckets[k % len(buckets)].add(p)\n",
        "        eff_users = len(buckets)\n",
        "\n",
        "    out = {b: set(i for p in buckets[b] for i in pat_to_idxs[p]) for b in range(eff_users)}\n",
        "    for b in range(eff_users, desired_users):  # padding\n",
        "        out[b] = set()\n",
        "    return out, eff_users\n",
        "\n",
        "def class_dist_for(idxs, labels_all, classes):\n",
        "    if len(idxs) == 0:\n",
        "        return {c: 0 for c in classes}\n",
        "    arr = np.array([labels_all[i] for i in idxs], dtype=int)\n",
        "    return dict(zip(classes, np.bincount(arr, minlength=len(classes)).tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHOBxS7eHbO6"
      },
      "outputs": [],
      "source": [
        "class ResNet50_client_side(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        # freeze early layers\n",
        "        for name, param in resnet.named_parameters():\n",
        "            if name.startswith((\"conv1\", \"bn1\")):\n",
        "                param.requires_grad = False\n",
        "        self.features = nn.Sequential(\n",
        "            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1, resnet.layer2\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "class ResNet50_server_side(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        self.features = nn.Sequential(resnet.layer3, resnet.layer4)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        num_ftrs = resnet.fc.in_features\n",
        "        self.classifier = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(num_ftrs, num_classes))\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, client_model, server_model):\n",
        "        super().__init__()\n",
        "        self.client_model=client_model\n",
        "        self.server_model=server_model\n",
        "    def forward(self, x):\n",
        "        return self.server_model(self.client_model(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foH4Bv3t2hWA"
      },
      "outputs": [],
      "source": [
        "class DenseNet169_client_side(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        dn = models.densenet169(weights=models.DenseNet169_Weights.DEFAULT)\n",
        "        feats = dn.features  # Sequential\n",
        "        # congela bem o começo\n",
        "        for n, p in dn.named_parameters():\n",
        "            if any(n.startswith(k) for k in [\"features.conv0\", \"features.norm0\"]):\n",
        "                p.requires_grad = False\n",
        "        self.features = nn.Sequential(\n",
        "            feats.conv0, feats.norm0, feats.relu0, feats.pool0,\n",
        "            feats.denseblock1, feats.transition1,\n",
        "            feats.denseblock2\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "class DenseNet169_server_side(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        dn = models.densenet169(weights=models.DenseNet169_Weights.DEFAULT)\n",
        "        feats = dn.features\n",
        "        self.features_tail = nn.Sequential(\n",
        "            feats.transition2,\n",
        "            feats.denseblock3, feats.transition3,\n",
        "            feats.denseblock4, feats.norm5\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        num_ftrs = dn.classifier.in_features  # 1664\n",
        "        self.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features_tail(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def make_models(backbone: str, num_classes: int):\n",
        "    b = backbone.lower()\n",
        "    if b == \"resnet50\":\n",
        "        net_glob_client = ResNet50_client_side().to(device)\n",
        "        net_glob_server = ResNet50_server_side(num_classes=num_classes).to(device)\n",
        "    elif b == \"densenet169\":\n",
        "        net_glob_client = DenseNet169_client_side().to(device)\n",
        "        net_glob_server = DenseNet169_server_side(num_classes=num_classes).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown backbone: {backbone}\")\n",
        "    return net_glob_client, net_glob_server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7DJ6aQqHs7-"
      },
      "outputs": [],
      "source": [
        "class BalancedBatchSampler(Sampler):\n",
        "    def __init__(self, dataset: CustomDataset, n_classes: int, n_per_class: int):\n",
        "        self.labels = [lbl for _, lbl in dataset]\n",
        "        self.class_to_idxs = defaultdict(list)\n",
        "        for i, y in enumerate(self.labels):\n",
        "            self.class_to_idxs[int(y)].append(i)\n",
        "\n",
        "        max_len = max((len(v) for v in self.class_to_idxs.values() if len(v) > 0), default=0)\n",
        "        if max_len == 0:\n",
        "            self.length = 0\n",
        "            self.n_classes = n_classes\n",
        "            self.n_per_class = n_per_class\n",
        "            return\n",
        "\n",
        "        for c in range(n_classes):\n",
        "            if len(self.class_to_idxs[c]) == 0:\n",
        "                for alt in range(n_classes):\n",
        "                    if len(self.class_to_idxs[alt]) > 0:\n",
        "                        self.class_to_idxs[c] = [self.class_to_idxs[alt][0]]\n",
        "                        break\n",
        "            while len(self.class_to_idxs[c]) < max_len:\n",
        "                self.class_to_idxs[c].extend(self.class_to_idxs[c])\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.n_per_class = n_per_class\n",
        "        self.length = max_len // n_per_class  # inteiro OK\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __iter__(self):\n",
        "        import random as _random\n",
        "        if self.length == 0:\n",
        "            return iter([])\n",
        "        class_iters = {c: iter(_random.sample(idxs, len(idxs))) for c, idxs in self.class_to_idxs.items()}\n",
        "        for _ in range(self.length):\n",
        "            batch = []\n",
        "            for c in range(self.n_classes):\n",
        "                for _ in range(self.n_per_class):\n",
        "                    try:\n",
        "                        batch.append(next(class_iters[c]))\n",
        "                    except StopIteration:\n",
        "                        class_iters[c] = iter(self.class_to_idxs[c])\n",
        "                        batch.append(next(class_iters[c]))\n",
        "            yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oltK0YrXbIzd"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self,patience=20,verbose=True,delta=0,save_path='chkp.pt'):\n",
        "        self.patience,self.verbose,self.delta,self.save_path=patience,verbose,delta,save_path\n",
        "        self.counter,self.best_score,self.early_stop,self.val_loss_min=0,None,False,np.Inf\n",
        "    def __call__(self,val_loss,models):\n",
        "        score=-val_loss; net_client, net_server = models\n",
        "        if self.best_score is None:\n",
        "            self.best_score=score; self.save_checkpoint(val_loss, net_client, net_server)\n",
        "        elif score<self.best_score+self.delta:\n",
        "            self.counter+=1\n",
        "            if self.verbose: print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
        "            if self.counter>=self.patience: self.early_stop=True\n",
        "        else:\n",
        "            self.best_score=score; self.save_checkpoint(val_loss, net_client, net_server); self.counter=0\n",
        "    def save_checkpoint(self,val_loss,net_client,net_server):\n",
        "        if self.verbose: print(f'Validation loss decreased ({self.val_loss_min:.6f}-->{val_loss:.6f}). Saving model to {self.save_path}')\n",
        "        torch.save({'net_glob_client_state_dict':net_client.state_dict(),\n",
        "                    'net_glob_server_state_dict':net_server.state_dict()}, self.save_path)\n",
        "        self.val_loss_min=val_loss\n",
        "\n",
        "def FedAvg_weighted(w_locals, counts):\n",
        "    \"\"\"Weighted average by client data size.\"\"\"\n",
        "    if not w_locals:\n",
        "        return {}\n",
        "    import copy as _copy, torch as _torch\n",
        "    N = float(sum(counts)) if sum(counts) > 0 else 1.0\n",
        "    w_avg = _copy.deepcopy(w_locals[0])\n",
        "    for k in w_avg.keys():\n",
        "        if not w_avg[k].dtype.is_floating_point:\n",
        "            continue\n",
        "        w_avg[k] = sum((w[k].float() * (n / N) for w, n in zip(w_locals, counts)))\n",
        "    return w_avg\n",
        "\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "def train_server(fx_client, y, net_server, optimizer_server, criterion, num_classes):\n",
        "    net_server.train(); optimizer_server.zero_grad()\n",
        "    fx_server = net_server(fx_client)\n",
        "    loss = criterion(fx_server, y)\n",
        "    # batch accuracy only\n",
        "    with torch.no_grad():\n",
        "        preds = fx_server.argmax(dim=1)\n",
        "        acc = F_tm.accuracy(preds, y, task='multiclass', num_classes=num_classes).item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(net_server.parameters(), max_norm=MAX_GRAD_NORM)\n",
        "    dfx_client = fx_client.grad.clone().detach()\n",
        "    optimizer_server.step()\n",
        "    return dfx_client, loss.item(), acc\n",
        "\n",
        "def evaluate_loader_aggregated(net_client, net_server, loader, criterion, num_classes, device):\n",
        "    net_client.eval(); net_server.eval()\n",
        "    all_logits, all_labels = [], []\n",
        "    total_loss, n_batches = 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            fx_client = net_client(images)\n",
        "            fx_server = net_server(fx_client)\n",
        "            loss = criterion(fx_server, labels)\n",
        "            total_loss += float(loss.item()); n_batches += 1\n",
        "            all_logits.append(fx_server.detach().cpu())\n",
        "            all_labels.append(labels.detach().cpu())\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0.0, [0.0]*5\n",
        "\n",
        "    logits = torch.cat(all_logits, dim=0)\n",
        "    y_true = torch.cat(all_labels, dim=0)\n",
        "    y_pred = logits.argmax(dim=1)\n",
        "\n",
        "    acc  = F_tm.accuracy(y_pred, y_true, task='multiclass', num_classes=num_classes).item()\n",
        "    prec = F_tm.precision(y_pred, y_true, average='macro', task='multiclass', num_classes=num_classes).item()\n",
        "    rec  = F_tm.recall(y_pred, y_true, average='macro', task='multiclass', num_classes=num_classes).item()\n",
        "    f1   = F_tm.f1_score(y_pred, y_true, average='macro', task='multiclass', num_classes=num_classes).item()\n",
        "    try:\n",
        "        auc = F_tm.auroc(torch.softmax(logits, dim=1), y_true, task='multiclass', num_classes=num_classes).item()\n",
        "    except Exception:\n",
        "        auc = 0.0\n",
        "\n",
        "    avg_loss = total_loss / n_batches\n",
        "    return avg_loss, [acc, prec, rec, f1, auc]\n",
        "\n",
        "def evaluate_accuracy(net,loader,device,return_conf_matrix=False,num_classes=3):\n",
        "    net.eval(); all_preds,all_labels,all_outputs=[],[],[]\n",
        "    with torch.no_grad():\n",
        "        for images,labels in loader:\n",
        "            images,labels=images.to(device),labels.to(device)\n",
        "            outputs=net(images); _,predicted=torch.max(outputs,1)\n",
        "            all_outputs.append(outputs.cpu())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    if len(all_preds) == 0:\n",
        "        if return_conf_matrix:\n",
        "            return 0,0,0,0,0, np.zeros((num_classes,num_classes), dtype=int)\n",
        "        return 0,0,0,0,0\n",
        "    all_outputs=torch.cat(all_outputs,dim=0)\n",
        "    all_preds=torch.tensor(all_preds); all_labels=torch.tensor(all_labels)\n",
        "    accuracy = F_tm.accuracy(all_preds,all_labels,task='multiclass',num_classes=num_classes).item()\n",
        "    precision=F_tm.precision(all_preds,all_labels,average='macro',task='multiclass',num_classes=num_classes).item()\n",
        "    recall  = F_tm.recall(all_preds,all_labels,average='macro',task='multiclass',num_classes=num_classes).item()\n",
        "    f1      = F_tm.f1_score(all_preds,all_labels,average='macro',task='multiclass',num_classes=num_classes).item()\n",
        "    try:\n",
        "        auc     = F_tm.auroc(F.softmax(all_outputs,dim=1),all_labels,task=\"multiclass\",num_classes=num_classes).item()\n",
        "    except ValueError:\n",
        "        auc = 0.0\n",
        "    cm=confusion_matrix(all_labels.cpu().numpy(),all_preds.cpu().numpy())\n",
        "    if return_conf_matrix: return accuracy,precision,recall,f1,auc,cm\n",
        "    return accuracy,precision,recall,f1,auc\n",
        "\n",
        "def _get_indices_from(dataset):\n",
        "    if isinstance(dataset, Subset):\n",
        "        return dataset.indices\n",
        "    if isinstance(dataset, CustomDataset) and isinstance(dataset.subset, Subset):\n",
        "        return dataset.subset.indices\n",
        "    raise ValueError(\"evaluate_by_patient: preciso de Subset(...) ou CustomDataset(Subset(...)).\")\n",
        "\n",
        "def evaluate_by_patient(model, dataset_or_subset, idx_to_patient_map, batch_size=64, device=\"cuda\"):\n",
        "    loader = DataLoader(dataset_or_subset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            logits = model(images)\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            all_probs.append(probs)\n",
        "            all_labels.extend(labels.numpy())\n",
        "    all_probs = np.concatenate(all_probs, axis=0)\n",
        "    original_indices = _get_indices_from(dataset_or_subset)\n",
        "    pats = [idx_to_patient_map[i] for i in original_indices]\n",
        "\n",
        "    df = pd.DataFrame(all_probs, columns=[f\"p_{c}\" for c in range(all_probs.shape[1])])\n",
        "    df[\"y\"] = np.array(all_labels, dtype=int)\n",
        "    df[\"patient\"] = pats\n",
        "\n",
        "    agg = df.groupby(\"patient\").agg({f\"p_{c}\":\"mean\" for c in range(all_probs.shape[1])})\n",
        "    y_true = df.groupby(\"patient\")[\"y\"].agg(lambda x: Counter(x).most_common(1)[0][0]).values\n",
        "    P = agg.values\n",
        "    y_pred = P.argmax(axis=1)\n",
        "\n",
        "    from sklearn import metrics as skm\n",
        "    acc  = (y_pred == y_true).mean().item()\n",
        "    prec = skm.precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    rec  = skm.recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    f1   = skm.f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    try:\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "        auc = roc_auc_score(y_true, P, multi_class=\"ovr\")\n",
        "    except Exception:\n",
        "        auc = 0.0\n",
        "    cm   = skm.confusion_matrix(y_true, y_pred, labels=list(range(P.shape[1])))\n",
        "    return acc, prec, rec, f1, auc, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmxFukaNwDKE"
      },
      "outputs": [],
      "source": [
        "class Client:\n",
        "    def __init__(self, idx, lr, device, idxs, idxs_test, net_server_initial_weights, weight_decay,\n",
        "                 batch_size, num_classes, criterion, mu, use_balanced_batch=True, epochs_total=80, local_ep=1):\n",
        "        self.idx,self.device,self.lr = idx,device,lr\n",
        "        self.local_ep = local_ep\n",
        "        self.num_classes = num_classes\n",
        "        self.criterion = criterion\n",
        "        self.mu = mu\n",
        "        self.epochs_total = epochs_total\n",
        "\n",
        "        self.train_dataset = CustomDataset(Subset(full_dataset, list(idxs)), transform=train_transform)\n",
        "        self.test_dataset  = CustomDataset(Subset(full_dataset, list(idxs_test)), transform=test_transform)\n",
        "\n",
        "        WORKERS = 2\n",
        "\n",
        "        labels_client = [lbl for _, lbl in self.train_dataset]\n",
        "        counts = Counter(labels_client)\n",
        "        n_per_class = max(1, batch_size // num_classes)\n",
        "        use_balanced = (len(np.unique(labels_client)) == num_classes) and (min(counts.values()) >= n_per_class)\n",
        "\n",
        "        if use_balanced and use_balanced_batch:\n",
        "            batch_sampler = BalancedBatchSampler(self.train_dataset, n_classes=num_classes, n_per_class=n_per_class)\n",
        "            try:\n",
        "                _ = next(iter(batch_sampler))\n",
        "                self.ldr_train = DataLoader(self.train_dataset, batch_sampler=batch_sampler,\n",
        "                                            num_workers=WORKERS, pin_memory=True)\n",
        "            except StopIteration:\n",
        "                self.ldr_train = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                            num_workers=WORKERS, pin_memory=True)\n",
        "        else:\n",
        "            self.ldr_train = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                        num_workers=WORKERS, pin_memory=True)\n",
        "\n",
        "        self.ldr_test = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                   num_workers=WORKERS, pin_memory=True)\n",
        "\n",
        "        self.net_server_local_copy = DenseNet169_server_side(num_classes=num_classes).to(self.device)\n",
        "        self.net_server_local_copy.load_state_dict(net_server_initial_weights)\n",
        "        self.optimizer_server_local = torch.optim.Adam(self.net_server_local_copy.parameters(),\n",
        "                                                       lr=self.lr, weight_decay=weight_decay)\n",
        "        self.scheduler_server = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer_server_local,\n",
        "                                                                           T_max=self.epochs_total, eta_min=1e-6)\n",
        "\n",
        "    def evaluate(self, net_client, net_server):\n",
        "        return evaluate_loader_aggregated(net_client, net_server, self.ldr_test,\n",
        "                                          self.criterion, self.num_classes, self.device)\n",
        "\n",
        "    def train(self, net_client, epoch_idx=None, progressive_unfreeze=False):\n",
        "        net_client.train(); net_client.to(self.device)\n",
        "\n",
        "        if progressive_unfreeze and epoch_idx is not None and epoch_idx >= 10:\n",
        "            for name, p in net_client.named_parameters():\n",
        "                if (\"denseblock2\" in name) or (\"layer2\" in name) or (\"client_model.5\" in name):\n",
        "                    p.requires_grad = True\n",
        "\n",
        "        client_params = []\n",
        "        low_lr, high_lr = 1e-5, self.lr\n",
        "        low_group, high_group = [], []\n",
        "        for n,p in net_client.named_parameters():\n",
        "            if not p.requires_grad:\n",
        "                continue\n",
        "            if (\"denseblock2\" in n) or (\"layer2\" in n) or (\"client_model.5\" in n):\n",
        "                low_group.append(p)\n",
        "            else:\n",
        "                high_group.append(p)\n",
        "        if low_group:  client_params.append({\"params\": low_group, \"lr\": low_lr})\n",
        "        if high_group: client_params.append({\"params\": high_group, \"lr\": high_lr})\n",
        "        if not client_params:\n",
        "            client_params = [{\"params\": [p for p in net_client.parameters() if p.requires_grad], \"lr\": self.lr}]\n",
        "\n",
        "        optimizer_client = torch.optim.Adam(client_params, weight_decay=1e-4)\n",
        "        scheduler_client = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_client,\n",
        "                                                                      T_max=self.epochs_total, eta_min=1e-6)\n",
        "\n",
        "        global_weights = copy.deepcopy(net_client.state_dict())\n",
        "\n",
        "        batch_losses, batch_accs = [], []\n",
        "        for _ in range(self.local_ep):\n",
        "            for images, labels in self.ldr_train:\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                optimizer_client.zero_grad()\n",
        "\n",
        "                fx = net_client(images)\n",
        "                client_fx = fx.clone().detach().requires_grad_(True)\n",
        "\n",
        "                dfx, loss, acc = train_server(client_fx, labels, self.net_server_local_copy,\n",
        "                                              self.optimizer_server_local, self.criterion, self.num_classes)\n",
        "\n",
        "                fx.backward(dfx)\n",
        "\n",
        "                if self.mu > 0.0:\n",
        "                    proximal_term = 0.0\n",
        "                    for name, param in net_client.named_parameters():\n",
        "                        if param.requires_grad:\n",
        "                            proximal_term += torch.sum((param - global_weights[name].to(self.device))**2)\n",
        "                    ((self.mu/2) * proximal_term).backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(net_client.parameters(), max_norm=1.0)\n",
        "                optimizer_client.step()\n",
        "\n",
        "                batch_losses.append(loss); batch_accs.append(float(acc))\n",
        "\n",
        "        scheduler_client.step(); self.scheduler_server.step()\n",
        "\n",
        "        avg_loss = sum(batch_losses)/len(batch_losses) if batch_losses else 0.0\n",
        "        avg_acc  = sum(batch_accs)/len(batch_accs)    if batch_accs  else 0.0\n",
        "        return net_client.state_dict(), self.net_server_local_copy.state_dict(), avg_loss, [avg_acc,0,0,0,0], len(self.train_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LT5i3T5wF1i"
      },
      "outputs": [],
      "source": [
        "def _safe_list(x):\n",
        "    return x if (isinstance(x, (list, tuple)) and len(x) > 0) else [0.0]\n",
        "\n",
        "def plot_history(history, num_users, save_dir=\"artifacts\"):\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "    epochs_range = range(1, len(_safe_list(history['train_acc'])) + 1)\n",
        "\n",
        "    # Accuracy\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_range, _safe_list(history['train_acc']), marker='o', linewidth=2, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, _safe_list(history['test_acc']),  marker='s', linewidth=2, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch', fontsize=13); plt.ylabel('Accuracy', fontsize=13)\n",
        "    plt.title(f'Accuracy over Epochs ({num_users} Clients)', fontsize=15)\n",
        "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.4); plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/accuracy_{num_users}c.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_range, _safe_list(history['train_loss']), marker='o', linewidth=2, label='Train Loss')\n",
        "    plt.plot(epochs_range, _safe_list(history['test_loss']),  marker='s', linewidth=2, label='Validation Loss')\n",
        "    plt.xlabel('Epoch', fontsize=13); plt.ylabel('Loss', fontsize=13)\n",
        "    plt.title(f'Loss over Epochs ({num_users} Clients)', fontsize=15)\n",
        "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.4); plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/loss_{num_users}c.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Recall\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_range, _safe_list(history['train_recall']), marker='o', linewidth=2, label='Train Recall (macro)')\n",
        "    plt.plot(epochs_range, _safe_list(history['test_recall']),  marker='s', linewidth=2, label='Validation Recall (macro)')\n",
        "    plt.xlabel('Epoch', fontsize=13); plt.ylabel('Recall', fontsize=13)\n",
        "    plt.title(f'Recall over Epochs ({num_users} Clients)', fontsize=15)\n",
        "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.4); plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/recall_{num_users}c.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # AUC\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_range, _safe_list(history['train_auc']), marker='o', linewidth=2, label='Train AUC (macro)')\n",
        "    plt.plot(epochs_range, _safe_list(history['test_auc']),  marker='s', linewidth=2, label='Validation AUC (macro)')\n",
        "    plt.xlabel('Epoch', fontsize=13); plt.ylabel('AUC', fontsize=13)\n",
        "    plt.title(f'AUC over Epochs ({num_users} Clients)', fontsize=15)\n",
        "    plt.legend(); plt.grid(True, linestyle='--', alpha=0.4); plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/auc_{num_users}c.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, num_users, save_dir=\"artifacts\", suffix=\"\"):\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cbar=True, cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label', fontsize=13)\n",
        "    plt.ylabel('True Label', fontsize=13)\n",
        "    plt.title(f'Confusion Matrix ({num_users} Clients){suffix}', fontsize=15)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_dir}/confusion_matrix_{num_users}c{suffix}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def summarize_distribution(dict_users, labels_all, classes, num_clients):\n",
        "    \"\"\"Build a DataFrame with per-class counts per client for one setup.\"\"\"\n",
        "    rows = []\n",
        "    for c in sorted(dict_users.keys()):\n",
        "        idxs = list(dict_users[c])\n",
        "        if len(idxs) == 0:\n",
        "            counts = {cls: 0 for cls in classes}\n",
        "        else:\n",
        "            labs = [int(labels_all[i]) for i in idxs]\n",
        "            binc = np.bincount(labs, minlength=len(classes)).tolist()\n",
        "            counts = dict(zip(classes, binc))\n",
        "        rows.append({\n",
        "            **counts,\n",
        "            \"client\": f\"C{c+1}\",\n",
        "            \"setup\":  f\"{num_clients} clients\",\n",
        "            \"group\":  f\"{num_clients}c-C{c+1}\"\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def plot_single_distribution_357(snapshots, labels_all, classes, save_path=\"artifacts/distribution_3_5_7_clients.png\"):\n",
        "    \"\"\"\n",
        "    Create ONE chart with class distributions per client\n",
        "    for the 3, 5 and 7 clients setups.\n",
        "    \"\"\"\n",
        "    dfs = []\n",
        "    for k in sorted(snapshots.keys()):  # k = 3, 5, 7 (if available)\n",
        "        df_k = summarize_distribution(snapshots[k], labels_all, classes, k)\n",
        "        dfs.append(df_k)\n",
        "    if not dfs:\n",
        "        print(\"[WARN] No distribution snapshot available for plotting.\")\n",
        "        return\n",
        "    df_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Long format\n",
        "    df_melt = df_all.melt(id_vars=[\"setup\", \"client\", \"group\"],\n",
        "                          value_vars=classes, var_name=\"Class\", value_name=\"Samples\")\n",
        "\n",
        "    # X-axis order: 3c-C1..C3, 5c-C1..C5, 7c-C1..C7\n",
        "    def _sort_key(g):\n",
        "        left, right = g.split(\"-C\")   # \"3c\", \"1\"\n",
        "        return (int(left.replace(\"c\",\"\").strip()), int(right))\n",
        "    df_melt = df_melt.sort_values(by=\"group\", key=lambda s: s.map(_sort_key))\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    sns.barplot(data=df_melt, x=\"group\", y=\"Samples\", hue=\"Class\", dodge=True)\n",
        "    plt.title(\"Class distribution per client for 3 / 5 / 7 clients\")\n",
        "    plt.xlabel(\"Setup–Client (e.g., 3c-C1 = 3 clients, client 1)\")\n",
        "    plt.ylabel(\"Number of samples\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"[OK] Distribution chart saved at: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qc7Rs0scIsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f013e11-83ac-4b7c-bd78-61b7ceb37aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STARTING TRAINING FOR 7 CLIENTS\n",
            "================================================================================\n",
            "\n",
            "[TRAIN] client 0: n=61  dist={'AD': 13, 'CN': 17, 'MCI': 31}\n",
            "[TEST ] client 0: n=19 dist={'AD': 1, 'CN': 8, 'MCI': 10}\n",
            "[TRAIN] client 1: n=59  dist={'AD': 14, 'CN': 24, 'MCI': 21}\n",
            "[TEST ] client 1: n=19 dist={'AD': 5, 'CN': 2, 'MCI': 12}\n",
            "[TRAIN] client 2: n=68  dist={'AD': 8, 'CN': 22, 'MCI': 38}\n",
            "[TEST ] client 2: n=26 dist={'AD': 5, 'CN': 2, 'MCI': 19}\n",
            "[TRAIN] client 3: n=44  dist={'AD': 9, 'CN': 14, 'MCI': 21}\n",
            "[TEST ] client 3: n=17 dist={'AD': 4, 'CN': 8, 'MCI': 5}\n",
            "[TRAIN] client 4: n=92  dist={'AD': 17, 'CN': 31, 'MCI': 44}\n",
            "[TEST ] client 4: n=18 dist={'AD': 1, 'CN': 7, 'MCI': 10}\n",
            "[TRAIN] client 5: n=48  dist={'AD': 14, 'CN': 19, 'MCI': 15}\n",
            "[TEST ] client 5: n=11 dist={'AD': 0, 'CN': 2, 'MCI': 9}\n",
            "[TRAIN] client 6: n=56  dist={'AD': 8, 'CN': 25, 'MCI': 23}\n",
            "[TEST ] client 6: n=11 dist={'AD': 0, 'CN': 2, 'MCI': 9}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train Loss: 1.0922 Acc: 0.3915 | Test  Loss: 1.1658 Acc: 0.2758 Rec: 0.3911 AUC: 0.4565 | Time: 69.95s\n",
            "Validation loss decreased (inf-->1.165845). Saving model to best_model_7_clients.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 | Train Loss: 1.0787 Acc: 0.4265 | Test  Loss: 1.1296 Acc: 0.3523 Rec: 0.3897 AUC: 0.4877 | Time: 24.06s\n",
            "Validation loss decreased (1.165845-->1.129592). Saving model to best_model_7_clients.pt\n",
            "Epoch 3/5 | Train Loss: 1.0883 Acc: 0.3952 | Test  Loss: 1.1082 Acc: 0.4046 Rec: 0.4610 AUC: 0.5084 | Time: 24.07s\n",
            "Validation loss decreased (1.129592-->1.108166). Saving model to best_model_7_clients.pt\n",
            "Epoch 4/5 | Train Loss: 1.0601 Acc: 0.4567 | Test  Loss: 1.0904 Acc: 0.4446 Rec: 0.4570 AUC: 0.5461 | Time: 24.53s\n",
            "Validation loss decreased (1.108166-->1.090390). Saving model to best_model_7_clients.pt\n",
            "Epoch 5/5 | Train Loss: 1.0419 Acc: 0.4963 | Test  Loss: 1.0712 Acc: 0.4902 Rec: 0.4126 AUC: 0.5619 | Time: 31.80s\n",
            "Validation loss decreased (1.090390-->1.071166). Saving model to best_model_7_clients.pt\n",
            "\n",
            "Training for 7 clients completed!\n",
            "\n",
            "--- Final Metrics (image-level, 7 Clients) ---\n",
            "Accuracy: 0.4793 | Precision: 0.4099 | Recall: 0.4189 | F1: 0.3960 | AUC: 0.6234\n",
            "--- Final Metrics (patient-level, 7 Clients) ---\n",
            "Accuracy: 0.5962 | Precision: 0.4903 | Recall: 0.5208 | F1: 0.4900 | AUC: 0.6514\n",
            "\n",
            "================================================================================\n",
            "STARTING TRAINING FOR 5 CLIENTS\n",
            "================================================================================\n",
            "\n",
            "[TRAIN] client 0: n=85  dist={'AD': 16, 'CN': 39, 'MCI': 30}\n",
            "[TEST ] client 0: n=27 dist={'AD': 1, 'CN': 14, 'MCI': 12}\n",
            "[TRAIN] client 1: n=93  dist={'AD': 10, 'CN': 30, 'MCI': 53}\n",
            "[TEST ] client 1: n=45 dist={'AD': 5, 'CN': 8, 'MCI': 32}\n",
            "[TRAIN] client 2: n=88  dist={'AD': 26, 'CN': 27, 'MCI': 35}\n",
            "[TEST ] client 2: n=14 dist={'AD': 5, 'CN': 3, 'MCI': 6}\n",
            "[TRAIN] client 3: n=81  dist={'AD': 18, 'CN': 25, 'MCI': 38}\n",
            "[TEST ] client 3: n=18 dist={'AD': 4, 'CN': 3, 'MCI': 11}\n",
            "[TRAIN] client 4: n=81  dist={'AD': 13, 'CN': 31, 'MCI': 37}\n",
            "[TEST ] client 4: n=17 dist={'AD': 1, 'CN': 3, 'MCI': 13}\n",
            "Epoch 1/5 | Train Loss: 1.0988 Acc: 0.3459 | Test  Loss: 1.1392 Acc: 0.3548 Rec: 0.3245 AUC: 0.4970 | Time: 28.71s\n",
            "Validation loss decreased (inf-->1.139222). Saving model to best_model_5_clients.pt\n",
            "Epoch 2/5 | Train Loss: 1.0704 Acc: 0.4469 | Test  Loss: 1.0844 Acc: 0.5064 Rec: 0.2941 AUC: 0.5686 | Time: 20.90s\n",
            "Validation loss decreased (1.139222-->1.084437). Saving model to best_model_5_clients.pt\n",
            "Epoch 3/5 | Train Loss: 1.0325 Acc: 0.4850 | Test  Loss: 1.0619 Acc: 0.5523 Rec: 0.3664 AUC: 0.6329 | Time: 22.01s\n",
            "Validation loss decreased (1.084437-->1.061869). Saving model to best_model_5_clients.pt\n",
            "Epoch 4/5 | Train Loss: 1.0067 Acc: 0.5434 | Test  Loss: 1.0292 Acc: 0.5466 Rec: 0.3061 AUC: 0.6507 | Time: 22.11s\n",
            "Validation loss decreased (1.061869-->1.029248). Saving model to best_model_5_clients.pt\n",
            "Epoch 5/5 | Train Loss: 0.9658 Acc: 0.5851 | Test  Loss: 1.0088 Acc: 0.5029 Rec: 0.2805 AUC: 0.6984 | Time: 29.25s\n",
            "Validation loss decreased (1.029248-->1.008785). Saving model to best_model_5_clients.pt\n",
            "\n",
            "Training for 5 clients completed!\n",
            "\n",
            "--- Final Metrics (image-level, 5 Clients) ---\n",
            "Accuracy: 0.4876 | Precision: 0.5196 | Recall: 0.2783 | F1: 0.2563 | AUC: 0.6731\n",
            "--- Final Metrics (patient-level, 5 Clients) ---\n",
            "Accuracy: 0.5192 | Precision: 0.1915 | Recall: 0.2812 | F1: 0.2278 | AUC: 0.7252\n",
            "\n",
            "================================================================================\n",
            "STARTING TRAINING FOR 3 CLIENTS\n",
            "================================================================================\n",
            "\n",
            "[TRAIN] client 0: n=174  dist={'AD': 24, 'CN': 63, 'MCI': 87}\n",
            "[TEST ] client 0: n=36 dist={'AD': 5, 'CN': 10, 'MCI': 21}\n",
            "[TRAIN] client 1: n=136  dist={'AD': 26, 'CN': 40, 'MCI': 70}\n",
            "[TEST ] client 1: n=42 dist={'AD': 6, 'CN': 11, 'MCI': 25}\n",
            "[TRAIN] client 2: n=118  dist={'AD': 33, 'CN': 49, 'MCI': 36}\n",
            "[TEST ] client 2: n=43 dist={'AD': 5, 'CN': 10, 'MCI': 28}\n",
            "Epoch 1/5 | Train Loss: 1.0141 Acc: 0.3607 | Test  Loss: 1.5306 Acc: 0.1327 Rec: 0.3333 AUC: 0.5956 | Time: 32.96s\n",
            "Validation loss decreased (inf-->1.530593). Saving model to best_model_3_clients.pt\n",
            "Epoch 2/5 | Train Loss: 0.9299 Acc: 0.4021 | Test  Loss: 1.4774 Acc: 0.1327 Rec: 0.3333 AUC: 0.6169 | Time: 24.02s\n",
            "Validation loss decreased (1.530593-->1.477409). Saving model to best_model_3_clients.pt\n",
            "Epoch 3/5 | Train Loss: 0.8712 Acc: 0.4885 | Test  Loss: 1.3735 Acc: 0.1327 Rec: 0.3333 AUC: 0.6270 | Time: 24.15s\n",
            "Validation loss decreased (1.477409-->1.373453). Saving model to best_model_3_clients.pt\n",
            "Epoch 4/5 | Train Loss: 0.8017 Acc: 0.5891 | Test  Loss: 1.1857 Acc: 0.2155 Rec: 0.3904 AUC: 0.6284 | Time: 24.01s\n",
            "Validation loss decreased (1.373453-->1.185670). Saving model to best_model_3_clients.pt\n",
            "Epoch 5/5 | Train Loss: 0.7387 Acc: 0.6389 | Test  Loss: 1.2329 Acc: 0.2264 Rec: 0.4204 AUC: 0.6369 | Time: 33.08s\n",
            "EarlyStopping counter: 1/20\n",
            "\n",
            "Training for 3 clients completed!\n",
            "\n",
            "--- Final Metrics (image-level, 3 Clients) ---\n",
            "Accuracy: 0.2149 | Precision: 0.4577 | Recall: 0.3909 | F1: 0.1877 | AUC: 0.6346\n",
            "--- Final Metrics (patient-level, 3 Clients) ---\n",
            "Accuracy: 0.2115 | Precision: 0.5379 | Recall: 0.4194 | F1: 0.2123 | AUC: 0.6861\n",
            "[OK] Distribution chart saved at: artifacts/distribuicao_3_5_7_clientes.png\n"
          ]
        }
      ],
      "source": [
        "client_counts = [3,5,7]  # scale experiments\n",
        "batch_size = 64\n",
        "epochs, frac, lr, weight_decay = 5, 1.0, 1e-4, 1e-4\n",
        "mu = 0.01  # FedProx (0.0 => FedAvg)\n",
        "progressive_unfreeze = True\n",
        "local_ep_default = 1\n",
        "\n",
        "# Class weights from global TRAIN\n",
        "labels_train_global = [label for _, label in main_train_dataset]\n",
        "present_classes = np.unique(labels_train_global)\n",
        "num_classes = len(classes)\n",
        "full_weights = np.ones((num_classes,), dtype=np.float32)\n",
        "if len(present_classes) > 0:\n",
        "    partial_weights = compute_class_weight(class_weight='balanced', classes=present_classes, y=labels_train_global)\n",
        "    for c, w in zip(present_classes, partial_weights):\n",
        "        full_weights[int(c)] = float(w)\n",
        "\n",
        "criterion_global = nn.CrossEntropyLoss(weight=torch.tensor(full_weights, dtype=torch.float32, device=device),\n",
        "                                       label_smoothing=0.05)\n",
        "\n",
        "dict_users_snapshots = {}\n",
        "\n",
        "for num_users in client_counts:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"STARTING TRAINING FOR {num_users} CLIENTS\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Partition train/test\n",
        "    dict_users, eff_train = partition_by_patient_balanced_no_empty(train_idx, IDX_TO_PATIENT, labels_all, num_users, seed=SEED)\n",
        "    dict_users_test, eff_test = partition_by_patient_balanced_no_empty(test_idx,  IDX_TO_PATIENT, labels_all, num_users, seed=SEED)\n",
        "\n",
        "    # Retry seeds if empties\n",
        "    tries = 0\n",
        "    while (any(len(dict_users[c])==0 for c in range(num_users))) and tries < 5:\n",
        "        tries += 1\n",
        "        alt_seed = SEED + 100*tries\n",
        "        dict_users, eff_train = partition_by_patient_balanced_no_empty(train_idx, IDX_TO_PATIENT, labels_all, num_users, seed=alt_seed)\n",
        "        dict_users_test, eff_test = partition_by_patient_balanced_no_empty(test_idx,  IDX_TO_PATIENT, labels_all, num_users, seed=alt_seed)\n",
        "\n",
        "    train_clients = [c for c in range(num_users) if len(dict_users[c]) > 0]\n",
        "    empty_clients = [c for c in range(num_users) if len(dict_users[c]) == 0]\n",
        "    if empty_clients:\n",
        "        print(f\"[Warn] Empty clients (ignored): {empty_clients}\")\n",
        "\n",
        "    # Audit print\n",
        "    for c in range(num_users):\n",
        "        tr_dist = class_dist_for(list(dict_users[c]), labels_all, classes)\n",
        "        te_dist = class_dist_for(list(dict_users_test[c]), labels_all, classes)\n",
        "        print(f\"[TRAIN] client {c}: n={len(dict_users[c])}  dist={tr_dist}\")\n",
        "        print(f\"[TEST ] client {c}: n={len(dict_users_test[c])} dist={te_dist}\")\n",
        "\n",
        "    dict_users_snapshots[num_users] = copy.deepcopy(dict_users)\n",
        "\n",
        "    # Models (DenseNet169 split)\n",
        "    net_glob_client = DenseNet169_client_side().to(device)\n",
        "    net_glob_server = DenseNet169_server_side(num_classes=num_classes).to(device)\n",
        "    w_glob_client = net_glob_client.state_dict()\n",
        "    w_glob_server = net_glob_server.state_dict()\n",
        "\n",
        "    history = {\n",
        "        'train_loss':[], 'train_acc':[], 'test_loss':[], 'test_acc':[],\n",
        "        'train_recall':[], 'test_recall':[], 'train_auc':[], 'test_auc':[]\n",
        "    }\n",
        "\n",
        "    checkpoint_path = f'best_model_{num_users}_clients.pt'\n",
        "    early_stopping = EarlyStopping(patience=20, verbose=True, save_path=checkpoint_path)\n",
        "\n",
        "    # ===== TRAIN LOOP =====\n",
        "    for iter_epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        m = max(int(frac * len(train_clients)), 1)\n",
        "        idxs_users = np.random.choice(train_clients, m, replace=False)\n",
        "\n",
        "        w_locals_client, w_locals_server, ns = [], [], []\n",
        "        round_train_loss, round_train_metrics = [], []\n",
        "\n",
        "        # STEP 1: LOCAL CLIENT TRAINING\n",
        "        for idx in idxs_users:\n",
        "            local = Client(idx, lr, device, dict_users[idx], dict_users_test[idx],\n",
        "                           w_glob_server, weight_decay, batch_size, num_classes, criterion_global, mu,\n",
        "                           use_balanced_batch=True, epochs_total=epochs, local_ep=local_ep_default)\n",
        "            w_c, w_s, t_loss, t_metrics, n_i = local.train(net_client=copy.deepcopy(net_glob_client),\n",
        "                                                           epoch_idx=iter_epoch,\n",
        "                                                           progressive_unfreeze=progressive_unfreeze)\n",
        "            w_locals_client.append(w_c)\n",
        "            w_locals_server.append(w_s)\n",
        "            ns.append(n_i)\n",
        "            round_train_loss.append(t_loss)\n",
        "            round_train_metrics.append(t_metrics)\n",
        "\n",
        "        # STEP 2: SERVER AGGREGATION\n",
        "        w_glob_client = FedAvg_weighted(w_locals_client, ns)\n",
        "        w_glob_server = FedAvg_weighted(w_locals_server, ns)\n",
        "        net_glob_client.load_state_dict(w_glob_client)\n",
        "        net_glob_server.load_state_dict(w_glob_server)\n",
        "\n",
        "        # STEP 3: GLOBAL EVALUATION\n",
        "        round_test_loss, round_test_metrics = [], []\n",
        "        for idx in idxs_users:\n",
        "            local = Client(idx, lr, device, dict_users[idx], dict_users_test[idx],\n",
        "                           w_glob_server, weight_decay, batch_size, num_classes, criterion_global, mu,\n",
        "                           use_balanced_batch=True, epochs_total=epochs)\n",
        "            test_loss, test_metrics = local.evaluate(net_glob_client, net_glob_server)\n",
        "            round_test_loss.append(test_loss)\n",
        "            round_test_metrics.append(test_metrics)\n",
        "\n",
        "        # History\n",
        "        history['train_loss'].append(sum(round_train_loss) / len(round_train_loss) if round_train_loss else 0.0)\n",
        "        if round_train_metrics:\n",
        "            avg_train_metrics = [sum(col) / len(col) for col in zip(*round_train_metrics)]\n",
        "        else:\n",
        "            avg_train_metrics = [0.0]*5\n",
        "        history['train_acc'].append(avg_train_metrics[0])\n",
        "        history['train_recall'].append(avg_train_metrics[2])\n",
        "        history['train_auc'].append(avg_train_metrics[4])\n",
        "\n",
        "        history['test_loss'].append(sum(round_test_loss) / len(round_test_loss) if round_test_loss else 0.0)\n",
        "        if round_test_metrics:\n",
        "            avg_test_metrics = [sum(col) / len(col) for col in zip(*round_test_metrics)]\n",
        "        else:\n",
        "            avg_test_metrics = [0.0]*5\n",
        "        history['test_acc'].append(avg_test_metrics[0])\n",
        "        history['test_recall'].append(avg_test_metrics[2])\n",
        "        history['test_auc'].append(avg_test_metrics[4])\n",
        "\n",
        "        # Confusion Matrix (Validation) periódico\n",
        "        if ((iter_epoch + 1) % CM_EVERY == 0) or (iter_epoch == 0):\n",
        "            y_true_all, y_pred_all = [], []\n",
        "            net_glob_client.eval(); net_glob_server.eval()\n",
        "            with torch.no_grad():\n",
        "                for idx in idxs_users:\n",
        "                    local_eval = Client(\n",
        "                        idx, lr, device, dict_users[idx], dict_users_test[idx],\n",
        "                        w_glob_server, weight_decay, batch_size, num_classes, criterion_global, mu,\n",
        "                        use_balanced_batch=True, epochs_total=epochs\n",
        "                    )\n",
        "                    for images, labels in local_eval.ldr_test:\n",
        "                        images = images.to(device)\n",
        "                        logits = net_glob_server(net_glob_client(images))\n",
        "                        preds  = logits.argmax(dim=1).cpu().numpy()\n",
        "                        y_pred_all.append(preds)\n",
        "                        y_true_all.append(labels.numpy())\n",
        "            if len(y_true_all) > 0:\n",
        "                y_true = np.concatenate(y_true_all)\n",
        "                y_pred = np.concatenate(y_pred_all)\n",
        "                cm_epoch = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
        "                plot_confusion_matrix(\n",
        "                    cm_epoch, classes, num_users,\n",
        "                    save_dir=\"artifacts\",\n",
        "                    suffix=f\"_epoch{iter_epoch+1:03d}\"\n",
        "                )\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {iter_epoch+1}/{epochs} | \"\n",
        "            f\"Train Loss: {history['train_loss'][-1]:.4f} Acc: {history['train_acc'][-1]:.4f} | \"\n",
        "            f\"Test  Loss: {history['test_loss'][-1]:.4f} \"\n",
        "            f\"Acc: {history['test_acc'][-1]:.4f} \"\n",
        "            f\"Rec: {history['test_recall'][-1]:.4f} \"\n",
        "            f\"AUC: {history['test_auc'][-1]:.4f} | \"\n",
        "            f\"Time: {time.time()-start_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "        early_stopping(history['test_loss'][-1], (net_glob_client, net_glob_server))\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            try:\n",
        "                y_true_all, y_pred_all = [], []\n",
        "                net_glob_client.eval(); net_glob_server.eval()\n",
        "                with torch.no_grad():\n",
        "                    for idx in idxs_users:\n",
        "                        local_eval = Client(\n",
        "                            idx, lr, device, dict_users[idx], dict_users_test[idx],\n",
        "                            w_glob_server, weight_decay, batch_size, num_classes, criterion_global, mu,\n",
        "                            use_balanced_batch=True, epochs_total=epochs\n",
        "                        )\n",
        "                        for images, labels in local_eval.ldr_test:\n",
        "                            images = images.to(device)\n",
        "                            logits = net_glob_server(net_glob_client(images))\n",
        "                            preds  = logits.argmax(dim=1).cpu().numpy()\n",
        "                            y_pred_all.append(preds)\n",
        "                            y_true_all.append(labels.numpy())\n",
        "                if len(y_true_all) > 0:\n",
        "                    y_true = np.concatenate(y_true_all)\n",
        "                    y_pred = np.concatenate(y_pred_all)\n",
        "                    cm_last = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
        "                    plot_confusion_matrix(cm_last, classes, num_users, save_dir=\"artifacts\", suffix=\"_last\")\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] Could not save last confusion matrix: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nTraining for {num_users} clients completed!\")\n",
        "    plot_history(history, num_users, save_dir=\"artifacts\")\n",
        "\n",
        "    # Avaliação final global (image-level)\n",
        "    try:\n",
        "        checkpoint_path = f'best_model_{num_users}_clients.pt'\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        net_glob_client.load_state_dict(checkpoint['net_glob_client_state_dict'])\n",
        "        net_glob_server.load_state_dict(checkpoint['net_glob_server_state_dict'])\n",
        "        final_model = CombinedModel(net_glob_client, net_glob_server).to(device)\n",
        "\n",
        "        test_loader_global = DataLoader(main_test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        acc, prec, rec, f1, auc, cm = evaluate_accuracy(final_model, test_loader_global,\n",
        "                                                        device, return_conf_matrix=True, num_classes=num_classes)\n",
        "\n",
        "        print(f\"\\n--- Final Metrics (image-level, {num_users} Clients) ---\")\n",
        "        print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "        plot_confusion_matrix(cm, classes, num_users, save_dir=\"artifacts\", suffix=\"_image\")\n",
        "\n",
        "        # Patient-level\n",
        "        try:\n",
        "            acc_p, prec_p, rec_p, f1_p, auc_p, cm_p = evaluate_by_patient(\n",
        "                final_model,\n",
        "                main_test_dataset,\n",
        "                IDX_TO_PATIENT,\n",
        "                batch_size=batch_size,\n",
        "                device=device\n",
        "            )\n",
        "            print(f\"--- Final Metrics (patient-level, {num_users} Clients) ---\")\n",
        "            print(f\"Accuracy: {acc_p:.4f} | Precision: {prec_p:.4f} | Recall: {rec_p:.4f} | F1: {f1_p:.4f} | AUC: {auc_p:.4f}\")\n",
        "            plot_confusion_matrix(cm_p, classes, num_users, save_dir=\"artifacts\", suffix=\"_patient\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Patient-level evaluation failed: {e}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[WARN] Checkpoint not found: {checkpoint_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Final evaluation failed: {e}\")\n",
        "\n",
        "try:\n",
        "    plot_single_distribution_357(\n",
        "        snapshots=dict_users_snapshots,\n",
        "        labels_all=labels_all,\n",
        "        classes=classes,\n",
        "        save_path=\"artifacts/distribuicao_3_5_7_clientes.png\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"[WARN] Falha ao gerar o gráfico único de distribuição: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}